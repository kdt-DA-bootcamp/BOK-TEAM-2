{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://search.hankyung.com/search/news?query=%EA%B8%88%EB%A6%AC&sort=RANK%2FDESC%2CDATE%2FDESC&period=DATE&area=ALL&sdate=2008.01.01&edate=2015.01.01&exact=%EA%B8%88%EB%A6%AC&include=&except=&hk_only=n&page=1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url, headers={}, data={})\n",
    "print(response.status_code) # 200 = ì •ìƒ\n",
    "print(response.text) # HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë§í¬ ë”° ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import concurrent.futures\n",
    "import csv\n",
    "\n",
    "# ğŸ”¹ ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ URL (í˜ì´ì§€ ë²ˆí˜¸ ì œì™¸)\n",
    "base_url = 'https://search.hankyung.com/search/news?query=%EA%B8%88%EB%A6%AC&sort=RANK%2FDESC%2CDATE%2FDESC&period=DATE&area=ALL&sdate=2010.01.01&edate=2025.02.27&exact=%EA%B8%88%EB%A6%AC&include=&except=%EA%B3%84%EC%A2%8C%2C%EC%9B%94%EB%93%9C%EC%BB%B5%2C%5B%ED%8F%AC%ED%86%A0%5D%2C%5B%ED%8E%80%EB%93%9C%EB%A7%A4%EB%8B%88%EC%A0%80%5D%2C%EC%83%81%EB%8B%B4%2C%EC%B6%94%EC%B2%9C&hk_only=y&page={}'  # í˜ì´ì§€ ë²ˆí˜¸ í¬ë§· ìˆ˜ì •\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "all_links = []  # ëª¨ë“  ê¸°ì‚¬ ë§í¬ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "# ğŸ”¹ í˜ì´ì§€ í¬ë¡¤ë§ í•¨ìˆ˜ ì •ì˜\n",
    "def crawl_page(page):\n",
    "    global all_links\n",
    "    search_url = base_url.format(page)  # í˜ì´ì§€ ë²ˆí˜¸ í¬ë§·íŒ…\n",
    "    try:\n",
    "        res = requests.get(search_url, headers=headers, timeout=5)\n",
    "        if res.status_code == 200:\n",
    "            soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "            # ğŸ”¹ ê¸°ì‚¬ ì œëª©ê³¼ ë§í¬ ê°€ì ¸ì˜¤ê¸°\n",
    "            articles = soup.find_all('em', class_='tit')\n",
    "            if articles:\n",
    "                for article in articles:\n",
    "                    link = article.find_parent('a')['href']\n",
    "                    all_links.append(link)\n",
    "\n",
    "            print(f\"í˜ì´ì§€ {page} í¬ë¡¤ë§ ì™„ë£Œ. í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜: {len(all_links)}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"í˜ì´ì§€ {page}ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "\n",
    "# ğŸ”¹ ë©€í‹°ìŠ¤ë ˆë”©ìœ¼ë¡œ í˜ì´ì§€ í¬ë¡¤ë§\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    # í˜ì´ì§€ ë²ˆí˜¸ 1ë¶€í„° 8589ê¹Œì§€\n",
    "    page_numbers = range(1, 8571)  # 8589ê¹Œì§€ í¬ë¡¤ë§\n",
    "    executor.map(crawl_page, page_numbers)\n",
    "\n",
    "# ğŸ”¹ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "csv_filename = 'news_links2.csv'\n",
    "with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Link\"])  # CSV íŒŒì¼ì˜ í—¤ë” ì‘ì„±\n",
    "\n",
    "    for link in all_links:\n",
    "        writer.writerow([link])  # ê° ë§í¬ë¥¼ CSV íŒŒì¼ì— ê¸°ë¡\n",
    "\n",
    "print(f\"\\nğŸ”¹ CSV íŒŒì¼ '{csv_filename}'ì— {len(all_links)}ê°œì˜ ë§í¬ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Link\n",
      "0  https://www.hankyung.com/article/2016080193475\n",
      "1  https://www.hankyung.com/article/2016080192518\n",
      "2  https://www.hankyung.com/article/2016080192308\n",
      "3  https://www.hankyung.com/article/2016080190768\n",
      "4  https://www.hankyung.com/article/2016080191425\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV íŒŒì¼ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì½ì–´ì˜¤ê¸°\n",
    "df = pd.read_csv('news_links.csv')\n",
    "\n",
    "# ë°ì´í„°í”„ë ˆì„ì˜ ì²˜ìŒ 5ê°œ í–‰ì„ ì¶œë ¥í•˜ì—¬ í™•ì¸\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë§í¬ íƒ€ê³  ë“¤ì–´ê°€ ê¸°ì‚¬ ê°–ê³  ì˜¤ê¸° title , contents , date ìˆœìœ¼ë¡œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ğŸ”¹ CSV íŒŒì¼ì—ì„œ ê¸°ì‚¬ ë§í¬ ì½ê¸°\n",
    "csv_file = 'news_links2.csv'\n",
    "df = pd.read_csv(csv_file, header=None)\n",
    "\n",
    "# ğŸ”¹ CSV ë°ì´í„° í™•ì¸ (ë””ë²„ê¹…ìš©)\n",
    "print(\"ğŸ” CSV ë°ì´í„° í”„ë¦¬ë·°:\")\n",
    "print(df.head())  # ì²˜ìŒ 5í–‰ ì¶œë ¥\n",
    "print(\"ğŸ” ì—´ ì¸ë±ìŠ¤:\", df.columns.tolist())  # ì—´ ë²ˆí˜¸ í™•ì¸\n",
    "\n",
    "# ğŸ”¹ ê¸°ì‚¬ ë§í¬ ë¦¬ìŠ¤íŠ¸ (í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì²˜ìŒ 5ê°œë§Œ ì‚¬ìš©)\n",
    "# ì—´ì´ í•˜ë‚˜ã—ã‹ãªã„ ê²½ìš° 0, ë‘ ë²ˆì§¸ ì—´ì„ ì›í•˜ë©´ 1ë¡œ ì„¤ì •\n",
    "links_column = 0  # â˜… ì—¬ê¸°ë¥¼ CSVì— ë§ê²Œ ìˆ˜ì • (0 ë˜ëŠ” 1 ë“±)\n",
    "links = df[links_column].tolist()[10001:83540]  # ì²˜ìŒ 5ê°œë§Œ ìŠ¬ë¼ì´ì‹±\n",
    "\n",
    "# ğŸ”¹ ë¶ˆí•„ìš”í•œ ê¸°ì ì •ë³´ ì œê±° í•¨ìˆ˜\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"([ê°€-í£]{2,4})\\s?(ê¸°ì|íŠ¹íŒŒì›)\", \"\", text)\n",
    "    text = re.sub(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", \"\", text)\n",
    "    text = re.sub(r\"\\(.*?\\)\", \"\", text)\n",
    "    text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "# ğŸ”¹ ê¸°ì‚¬ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def scrape_article(link):\n",
    "    try:\n",
    "        res = requests.get(link, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)\n",
    "        if res.status_code != 200:\n",
    "            print(f\"âŒ ìš”ì²­ ì‹¤íŒ¨: {res.status_code} - {link}\")\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        # âœ… ì œëª© ê°€ì ¸ì˜¤ê¸°\n",
    "        title = soup.select_one(\"h1.headline\")\n",
    "        title = title.text.strip() if title else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "        # âœ… ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°\n",
    "        date = soup.select_one(\"span.txt-date\")\n",
    "        date = date.text.strip() if date else \"ë‚ ì§œ ì—†ìŒ\"\n",
    "        date = re.sub(r'\\d{2}:\\d{2}', '', date).strip()\n",
    "        date_match = re.search(r\"(\\d{4})ë…„ (\\d{1,2})ì›” (\\d{1,2})ì¼\", date)\n",
    "        if date_match:\n",
    "            date = f\"{date_match.group(1)}-{date_match.group(2).zfill(2)}-{date_match.group(3).zfill(2)}\"\n",
    "\n",
    "        # âœ… ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°\n",
    "        article_body = soup.find(\"div\", class_=\"article-body\", id=\"articletxt\")\n",
    "        if article_body:\n",
    "            contents = \"\\n\".join([element for element in article_body.stripped_strings])\n",
    "            contents = clean_text(contents)\n",
    "        else:\n",
    "            contents = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "        return {'title': title, 'contents': contents, 'date': date}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"ğŸš¨ ì˜¤ë¥˜ ë°œìƒ: {link} - {e}\")\n",
    "        return None\n",
    "\n",
    "# ğŸ”¹ ë©€í‹°ìŠ¤ë ˆë”© í¬ë¡¤ë§ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©)\n",
    "def main():\n",
    "    print(f\"ğŸ” í…ŒìŠ¤íŠ¸: {len(links)}ê°œì˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘...\")\n",
    "\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=15) as executor:\n",
    "        futures = {executor.submit(scrape_article, link): link for link in links}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                results.append(result)\n",
    "                print(f\"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {result['title']}\")\n",
    "\n",
    "    # ğŸ”¹ í¬ë¡¤ë§ ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜ í›„ CSV ì €ì¥\n",
    "    output_df = pd.DataFrame(results, columns=['title', 'contents', 'date'])\n",
    "    output_df.to_csv('test_scraped_articles2.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(\"ğŸ“ í…ŒìŠ¤íŠ¸ ì™„ë£Œ! 'test_scraped_articles2.csv' íŒŒì¼ ì €ì¥ ì™„ë£Œ.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê°œí–‰ ë¬¸ì œ í•´ê²° ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ 'ì œëª© ì—†ìŒ'ì´ ì œê±°ëœ CSV íŒŒì¼ì´ 'filtered_articles_real.csv'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ğŸ”¹ ê¸°ì¡´ CSV íŒŒì¼ ì½ê¸°\n",
    "input_csv_file = 'test_scraped_articles.csv'\n",
    "df = pd.read_csv(input_csv_file)\n",
    "\n",
    "# ğŸ”¹ ì œëª©ì´ 'ì œëª© ì—†ìŒ'ì¸ í–‰ ì œê±°\n",
    "df_filtered = df[df['title'] != 'ì œëª© ì—†ìŒ']\n",
    "\n",
    "# ğŸ”¹ ê²°ê³¼ë¥¼ ìƒˆë¡œìš´ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "output_csv_file = 'filtered_articles_real.csv'\n",
    "df_filtered.to_csv(output_csv_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"ğŸ‰ 'ì œëª© ì—†ìŒ'ì´ ì œê±°ëœ CSV íŒŒì¼ì´ '{output_csv_file}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ ê°œí–‰ ë¬¸ì œ í•´ê²°! ìƒˆë¡œìš´ íŒŒì¼: 'filtered_articles_fixed.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv  # CSV ì˜µì…˜ ì¶”ê°€\n",
    "\n",
    "# ğŸ”¹ ê¸°ì¡´ CSV íŒŒì¼ ì½ê¸°\n",
    "input_csv_file = 'filtered_articles_real.csv'\n",
    "df = pd.read_csv(input_csv_file)\n",
    "\n",
    "# ğŸ”¹ CSV ì €ì¥ ì‹œ ê°œí–‰ ë¬¸ì œ í•´ê²°\n",
    "output_csv_file = 'filtered_articles_fixed.csv'\n",
    "df.to_csv(output_csv_file, index=False, encoding='utf-8-sig', lineterminator='\\n', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "print(f\"ğŸ‰ ê°œí–‰ ë¬¸ì œ í•´ê²°! ìƒˆë¡œìš´ íŒŒì¼: '{output_csv_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(output_csv_file, index=False, encoding='utf-8-sig', lineterminator='\\n', quoting=csv.QUOTE_ALL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"contents\"] = df[\"contents\"].astype(str).replace(\"\\n\", \" \", regex=True)\n",
    "df.to_csv(output_csv_file, index=False, encoding='utf-8-sig', lineterminator='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê¸°ì‚¬ ì •ì œ ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë³¸ ë°ì´í„° ê°œìˆ˜: 9026\n",
      "í•„í„°ë§ í›„ ë°ì´í„° ê°œìˆ˜: 9021\n",
      "í•„í„°ë§ëœ ë°ì´í„°ë¥¼ '20.csv' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ğŸ”¹ CSV íŒŒì¼ ì½ê¸° (ì—´ ì´ë¦„ ì—†ì´ ì½ê¸°)\n",
    "csv_file = '20.csv'  # CSV íŒŒì¼ ê²½ë¡œ\n",
    "df = pd.read_csv(csv_file, header=None)  # header=Noneìœ¼ë¡œ ì—´ ì´ë¦„ ì—†ì´ ì½ê¸°\n",
    "\n",
    "# ğŸ”¹ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ê¸°ì¡´ í‚¤ì›Œë“œ + ì¶”ê°€ í‚¤ì›Œë“œ)\n",
    "keywords = [\n",
    "    \"ì„ì˜ì›…\", \"í‹°ëª¬\", \"ìœ„ë©”í”„\", \"í‹°ë©”í”„\", \"ì•±\", \"ë‚±ë§\", \"ê³„ì¢Œ\", \"ë‹¬ë¹›\", \"ì ê¸ˆ\", \"ì•„ì´í°\", \"tkg\", \"ì—­ì„¸ê¶Œ\", \"í† ìŠ¤\", \"í•™ì›\", \"ë†í˜‘\", \"ì¬ê°œë°œ\", \n",
    "    \"ë¹„íŠ¸ì½”ì¸\", \"ê³µì¸\", \"ë¶€ì‚°ì€í–‰\", \"ì•±\", \"ë°°ë‹¬\", \"ê±´ê°•\", \"ë³´í—˜\", \"ë¶„ì–‘\", \"ì…ì£¼\", \"ì² ë„\", \"AI\", \"ë„¤ì´ë²„\", \"í•œìƒ˜\", \"ì•”í˜¸\", \"ì½”ì¸\", \"í˜„ëŒ€\", \n",
    "    \"ì‚¼ì„±\", \"ì¤‘ê³ ì°¨\", \"í”¼ì•„ë…¸\", \"ë°±í™”ì \", \"ë³¼ë³´\", \"ìœ„ìŠ¤í‚¤\", \"ì¹´ë“œ\", \"íŒ¨ë”©\", \"ë³€ì‹ \", \"íˆ¬ì\", \"ìŠ¤íƒë¡ \", \"ì¹´ë±…\", \"ë‹¤ì´ì†Œ\", \"ë©¤ë²„ì‹­\", \"ë¬¼ëŸ‰\", \n",
    "    \"ë£¨ì´ë¹„í†µ\", \"ì¦ê¶Œ\", \"ìˆ˜ìµ\", \"skt\", \"kt\", \"ì‹ í•œ\", \"ìš°ë¦¬\", \"í•˜ë‚˜\", \"ì¶œì‹œ\", \"í˜„ëŒ€ì°¨\", \"ê°•ë‚¨\", \"ì½˜ì„œíŠ¸\", \"ê±´ì„¤ì¡°í•©\", \"ìº í˜ì¸\", \"ìš”ê¸°ìš”\", \n",
    "    \"ë°°ë‹¬\", \"í­ìš°\", \"í‘œì°½\", \"ìì—°ì¬í•´\", \"lgìœ í”ŒëŸ¬ìŠ¤\", \"ê°€ì§œ\", \"ì‚¬ë£Œê°’\", \"ë³´ì´ìŠ¤\", \"ë§¨í•´íŠ¼\", \"ë¸”ë¡œê·¸\", \"ì˜¨íƒíŠ¸\", \"ë°•ëŒíšŒ\", \"ì œë„¤ì‹œìŠ¤\", \n",
    "    \"ê¸°ì•„\", \"ì‹ ì°¨\", \"ì˜¤í”¼ìŠ¤í…”\", \"ìŠ¤ë§ˆíŠ¸í°\", \"esg\", \"ì´ë¯¼\", \"ì›°ì»´ì €ì¶•\", \"ì¥ë³‘\", \"ë‘”ì´Œ\", \"ì£¼ê³µ\", \"ì €íƒ„ì†Œ\", \"í‚¨í…ìŠ¤\", \"ìŠ¤íƒ€íŠ¸ì—…\", \"lh\", \n",
    "    \"ë¡¯ë°\", \"ì–¸ë”ì•„ë¨¸\", \"ì¼€ì´ë±…í¬\", \"ë„¤ì´ë²„í˜ì´\", \"íŒë§¤\", \"í¼ì¦\", \"ë‹¤ì˜¬\", \"ì„ ì°©ìˆœ\", \"ì¶œì‹œ\", \"í† ì§€ê±°ë˜í—ˆê°€\", \"ê°€ìŠ¤ê³µê¸‰\", \"sbi\", \"ì…ì \", \n",
    "    \"í˜ì´ì½”\", \"ê²½í’ˆ\", \"ì—ì–´íŒŸ\", \"ê³¨ë“œë°”\", \"ì›”ì„¸\", \"ë¥´ë…¸\", \"ìˆ˜í˜‘\", \"ì¶•í˜‘\", \"ì‹ í˜¼\", \"íƒ€ìš´\", \"ìš°ëŒ€í˜•\", \"ë‹¤ë°©\", \"ë¬´ë£Œì¡°íšŒ\", \"ìš°ë¦¬ì¹´ë“œ\", \n",
    "    \"ì—¬í–‰\", \"ì €ì†Œë“ì¸µ\", \"ì„ëŒ€ì£¼íƒ\", \"ê¸‰ì „\", \"ì‹¤ì¢…\", \"ì—¬ëŒ€ìƒ\", \"ë‚¨ìì¹œêµ¬\", \"ì†Œì¬\", \"ì•„íŒŒíŠ¸\", \"í•€í…Œí¬\",  \"ê²Œì„\", \"ì˜í™”\", \"ë“œë¼ë§ˆ\", \"ì˜ˆëŠ¥\", \"ìŒì•…\", \"ì½˜ì„œíŠ¸\", \"íŒ¬ë¯¸íŒ…\", \"ìœ íŠœë¸Œ\", \"ì¸í”Œë£¨ì–¸ì„œ\", \"íŒ¨ì…˜\", \"ë·°í‹°\", \"ë©”ì´í¬ì—…\", \"í—¤ì–´\", \"ìŠ¤íƒ€ì¼\", \n",
    "    \"ìš´ë™\", \"í—¬ìŠ¤\", \"ìš”ê°€\", \"í•„ë¼í…ŒìŠ¤\", \"ë“±ì‚°\", \"ê³¨í”„\", \"ì¶•êµ¬\", \"ì•¼êµ¬\", \"ë†êµ¬\", \"í…Œë‹ˆìŠ¤\", \"ìŠ¤í‚¤\", \"ìŠ¤ë…¸ìš°ë³´ë“œ\", \"ì—¬í–‰\", \"ë§›ì§‘\", \"ì¹´í˜\", \n",
    "    \"ë””ì €íŠ¸\", \"ì™€ì¸\", \"ë§¥ì£¼\", \"ì¹µí…Œì¼\", \"ì»¤í”¼\", \"ì°¨\", \"ë°˜ë ¤ë™ë¬¼\", \"ê°•ì•„ì§€\", \"ê³ ì–‘ì´\", \"ì• ì™„ë™ë¬¼\", \"ë™ë¬¼ë³‘ì›\", \"ë™ë¬¼ì›\", \"ê³µì›\", \"ì „ì‹œíšŒ\", \n",
    "    \"ë°•ë¬¼ê´€\", \"ë¯¸ìˆ ê´€\", \"ê³µì—°\", \"ë®¤ì§€ì»¬\", \"ì—°ê·¹\", \"ì˜¤í˜ë¼\", \"ë°œë ˆ\", \"í´ë˜ì‹\", \"ì¬ì¦ˆ\", \"ë½\", \"í™í•©\", \"K-pop\", \"ì•„ì´ëŒ\", \"ë°°ìš°\", \"ê°€ìˆ˜\", \n",
    "    \"ì‘ê°€\", \"í™”ê°€\", \"ì‚¬ì§„\", \"ì´¬ì˜\", \"ë“œë¡ \", \"VR\", \"AR\", \"ë©”íƒ€ë²„ìŠ¤\", \"ê²Œì„\", \"eìŠ¤í¬ì¸ \", \"í”„ë¡œê²Œì´ë¨¸\", \"ìŠ¤ë§ˆíŠ¸í°\", \"íƒœë¸”ë¦¿\", \"ë…¸íŠ¸ë¶\", \n",
    "    \"ë°ìŠ¤í¬íƒ‘\", \"í”„ë¦°í„°\", \"ì¹´ë©”ë¼\", \"ìŠ¤í”¼ì»¤\", \"ì´ì–´í°\", \"ìŠ¤ë§ˆíŠ¸ì›Œì¹˜\", \"ê°€ì „ì œí’ˆ\", \"ëƒ‰ì¥ê³ \", \"ì„¸íƒê¸°\", \"ì—ì–´ì»¨\", \"ì²­ì†Œê¸°\", \"ë¡œë´‡\", \"ë“œë¡ \", \n",
    "    \"ìŠ¤ë§ˆíŠ¸í™ˆ\", \"IoT\", \"ì¸ê³µì§€ëŠ¥\", \"ë¨¸ì‹ ëŸ¬ë‹\", \"ë”¥ëŸ¬ë‹\", \"ë¸”ë¡ì²´ì¸\", \"ê°€ìƒí™”í\", \"NFT\", \"ë©”íƒ€ë²„ìŠ¤\", \"ìŠ¤íƒ€íŠ¸ì—…\", \"ì°½ì—…\", \"ë²¤ì²˜\", \"íˆ¬ì\", \n",
    "    \"í¬ë¼ìš°ë“œí€ë”©\", \"ì‡¼í•‘\", \"ì˜¨ë¼ì¸ì‡¼í•‘\", \"ì˜¤í”„ë¼ì¸ì‡¼í•‘\", \"í• ì¸\", \"ì´ë²¤íŠ¸\", \"ê²½í’ˆ\", \"ì¶”ì²¨\", \"ê³µëª¨ì „\", \"ëŒ€íšŒ\", \"ìˆ˜ìƒ\", \"ì‹œìƒì‹\", \"ì—°ì˜ˆì¸\", \n",
    "    \"ìœ ëª…ì¸\", \"ì¸ìŠ¤íƒ€ê·¸ë¨\", \"í˜ì´ìŠ¤ë¶\", \"íŠ¸ìœ„í„°\", \"SNS\", \"ì†Œì…œë¯¸ë””ì–´\", \"ë¸”ë¡œê·¸\", \"ìœ íŠœë²„\", \"ìŠ¤íŠ¸ë¦¬ë¨¸\", \"ë°©ì†¡\", \"ë¼ë””ì˜¤\", \"íŒŸìºìŠ¤íŠ¸\", \n",
    "    \"ì›¹íˆ°\", \"ë§Œí™”\", \"ì• ë‹ˆë©”ì´ì…˜\", \"ì˜í™”ì œ\", \"ì˜í™”í‰ë¡ \", \"ì˜í™”ë¦¬ë·°\", \"ë“œë¼ë§ˆë¦¬ë·°\", \"ì˜ˆëŠ¥ë¦¬ë·°\", \"ìŒì•…ë¦¬ë·°\", \"ì±…\", \"ì†Œì„¤\", \"ì‹œ\", \"ì—ì„¸ì´\", \n",
    "    \"ë…ì„œ\", \"ì„œì \", \"ë„ì„œê´€\", \"í•™ìŠµ\", \"êµìœ¡\", \"ì˜¨ë¼ì¸ê°•ì˜\", \"ì˜¤í”„ë¼ì¸ê°•ì˜\", \"í•™ì›\", \"ê³¼ì™¸\", \"ìœ í•™\", \"ì–´í•™ì—°ìˆ˜\", \"ì™¸êµ­ì–´\", \"ì˜ì–´\", \n",
    "    \"ì¼ë³¸ì–´\", \"ì¤‘êµ­ì–´\", \"í”„ë‘ìŠ¤ì–´\", \"ìŠ¤í˜ì¸ì–´\", \"ë…ì¼ì–´\", \"ì´íƒˆë¦¬ì•„ì–´\", \"ëŸ¬ì‹œì•„ì–´\", \"ì•„ëì–´\", \"íŒë””ì–´\", \"í¬ë¥´íˆ¬ê°ˆì–´\", \"ë„¤ëœë€ë“œì–´\", \n",
    "    \"ìŠ¤ì›¨ë´ì–´\", \"ë…¸ë¥´ì›¨ì´ì–´\", \"ë´ë§ˆí¬ì–´\", \"í•€ë€ë“œì–´\", \"ê·¸ë¦¬ìŠ¤ì–´\", \"í„°í‚¤ì–´\", \"ë² íŠ¸ë‚¨ì–´\", \"íƒœêµ­ì–´\", \"ì¸ë„ë„¤ì‹œì•„ì–´\", \"ë§ë ˆì´ì‹œì•„ì–´\", \n",
    "    \"í•„ë¦¬í•€ì–´\", \"ì‹±ê°€í¬ë¥´ì–´\", \"í™ì½©ì–´\", \"ëŒ€ë§Œì–´\", \"í•œêµ­ì–´\", \"í•œê¸€\", \"í•œêµ­ë¬¸í™”\", \"K-pop\", \"K-drama\", \"K-beauty\", \"K-food\", \"í•œì‹\", \n",
    "    \"ê¹€ì¹˜\", \"ë¶ˆê³ ê¸°\", \"ë¹„ë¹”ë°¥\", \"ë–¡ë³¶ì´\", \"ë¼ë©´\", \"ì‚¼ê²¹ì‚´\", \"ê°ˆë¹„\", \"ì¡±ë°œ\", \"ë³´ìŒˆ\", \"ëƒ‰ë©´\", \"ìˆœëŒ€\", \"ë–¡\", \"í•œê³¼\", \"ì „í†µìŒì‹\", \"ì „í†µë¬¸í™”\", \n",
    "    \"ì „í†µì˜ìƒ\", \"í•œë³µ\", \"ì „í†µì¶¤\", \"ì „í†µìŒì•…\", \"ì‚¬ë¬¼ë†€ì´\", \"íŒì†Œë¦¬\", \"ë¯¼ìš”\", \"ì „í†µì•…ê¸°\", \"ê°€ì•¼ê¸ˆ\", \"í•´ê¸ˆ\", \"ê±°ë¬¸ê³ \", \"ëŒ€ê¸ˆ\", \"ì†Œê¸ˆ\", \n",
    "    \"í”¼ë¦¬\", \"ì¥êµ¬\", \"ë¶\", \"ì§•\", \"ê½¹ê³¼ë¦¬\", \"ì†Œê³ \", \"í’ë¬¼\", \"ë†ì•…\", \"ë¯¼ì†ë†€ì´\", \"ìœ·ë†€ì´\", \"ë„ë›°ê¸°\", \"ì œê¸°ì°¨ê¸°\", \"ì—°ë‚ ë¦¬ê¸°\", \"ì”¨ë¦„\", \n",
    "    \"íƒœê¶Œë„\", \"í•©ê¸°ë„\", \"ìœ ë„\", \"ê²€ë„\", \"ê¶ë„\", \"ì „í†µë¬´ìˆ \", \"ì „í†µì˜ˆìˆ \", \"ì „í†µê³µì˜ˆ\", \"ë„ìê¸°\", \"ì²­ì\", \"ë°±ì\", \"ë¶„ì²­ì‚¬ê¸°\", \"ë„ì˜ˆ\", \n",
    "    \"ëª©ê³µì˜ˆ\",\"ìˆ˜ì§€\",\"mcëª½\",\"í˜ì˜¤\",\"ê¹€í¬ì„ \",\"ì˜ˆëŠ¥\",\"ì˜ì—…ìµ\",\"ì˜¥íƒ€ê³¤\",\"ì§€ë“œë˜ê³¤\",\"ìœ ì´\",\"ì—”ë”©\",\"ë°°ìˆ˜ì •\",\"ë°°ìš©ì¤€\",\"kbs\",\"ì‚¼ë””\",\"ê¹€í˜œì\",\"ìœ ì¬ì„\",\"ì•„ì´ìœ \",\"ì˜ˆëŠ¥\",\"ì´ë™ìš±\",\"ë“œë¼ë§ˆ\",\"ë°°ì„±ì¬\",\"ì•„ë‚˜ìš´ì„œ\",\"sbs\",\"mbc\",\"ë³„ì—ì„œ\",\"í•œê·¸ë£¨\"\n",
    "     \"ë°•ëª…ìˆ˜\",\"ê²¨ìš¸ë°©í•™\",\"ì—¬ë“œë¦„\",\"ìœ ì•„ì¸\",\"ë¹„íƒ€ë¯¼\",\"ì…€ì¹´\",\"ë² ì´ê¸€\",\"ë°•ì‹ í˜œ\",\"ì¥ìˆ˜ì›\",\"ì—°ì¸\",\"ì´¬ì˜\",\"í¥í–‰\",\"ì˜ˆì›\",\"ì •ì¤€í•˜\",\"ê¹€ìˆ˜í˜„\",\"ì´ì˜ì\",\"JYJ\",\"ê¹€ì„±ê· \",\"ì´ì¢…ì„\",\"ì…€ì¹´ê°€\",\"ìš°ìŠ¹\",\"ì‚¬ê³¼\",\"ì œì•½\",\"ë³„ê·¸ëŒ€\",\"SM\",\"ë¯¸ë…€\"\n",
    "      \"ì„ìˆ˜í–¥\",\"ì§€ì§„í¬\",\"LGìœ í”ŒëŸ¬ìŠ¤\",\"ì˜¬ë¦¼í”½\",\"ì´ê·œí˜\",\"ì‹ ëŒ€ì² \",\"ì—‘ì†Œ\",\"ê¹€ê²½ë€\",\"ê¹€íƒœí¬\",\"ìŠˆí¼ë§¨\",\"ì§„ì§œì‚¬ë‚˜ì´\",\"ë¡œë˜\",\"ì†¡ì¬ë¦¼\",\"ê³ ì•„ì„±\",\"í˜œë ¹\",\"êµ­ì¹´ìŠ¤í…\",\"ì‹¬ê¸ˆ\",\"ì‹œì²­ë¥ \",\"íˆë“ ì‹±ì–´\",\"ë¹„ì—¼\",\"ì„ì‹ \",\"TV\", \"ê°€ì¡±ë¼ë¦¬\",\"ì„±íƒ„ì ˆ\",\" ë‚˜ë‚˜\",\"`ì „ì„¤ì˜ ë§ˆë…€`\",\n",
    "       \"â€˜ì§„ì§œ ì‚¬ë‚˜ì´â€™\",\"`ëŸ°ë‹ë§¨`\",\"ì†¡ì¼êµ­\",\"`íë§ìº í”„`\",\"ì—´ì• \",\"`ë¯¸ìƒ`\",\"`ì£½ê¸°ì „ì—\",\"ì£½ê¸° ì „ì—\" ,\"ë¡œë§¨ìŠ¤ì˜\",\"ìš°ì§€ì›-ì´êµì˜\",\"ì‹ í•´ì² \",\"`ë»ê¾¸ê¸° ë‘¥ì§€`\"# ì¶”ê°€ í‚¤ì›Œë“œ\n",
    "]\n",
    "\n",
    "# ğŸ”¹ í‚¤ì›Œë“œ í•„í„°ë§ í•¨ìˆ˜\n",
    "def contains_keyword(text, keywords):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì— í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):  # NaN ê°’ ì²˜ë¦¬\n",
    "        return False\n",
    "    return any(keyword in text for keyword in keywords)\n",
    "\n",
    "# ğŸ”¹ í•„í„°ë§ëœ ë§í¬ ì¶”ì¶œ\n",
    "# ì œëª©(0ë²ˆ ì—´)ì—ì„œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ í–‰ ì œì™¸\n",
    "filtered_df = df[~df[0].apply(lambda x: contains_keyword(x, keywords))]\n",
    "\n",
    "# ğŸ”¹ ê²°ê³¼ í™•ì¸\n",
    "print(f\"ì›ë³¸ ë°ì´í„° ê°œìˆ˜: {len(df)}\")\n",
    "print(f\"í•„í„°ë§ í›„ ë°ì´í„° ê°œìˆ˜: {len(filtered_df)}\")\n",
    "\n",
    "# ğŸ”¹ í•„í„°ë§ëœ ë°ì´í„° ì €ì¥\n",
    "filtered_csv_file = '20.csv'  # ì €ì¥í•  íŒŒì¼ ê²½ë¡œ\n",
    "filtered_df.to_csv(filtered_csv_file, index=False, header=False)  # header=Falseë¡œ ì—´ ì´ë¦„ ì—†ì´ ì €ì¥\n",
    "print(f\"í•„í„°ë§ëœ ë°ì´í„°ë¥¼ '{filtered_csv_file}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
