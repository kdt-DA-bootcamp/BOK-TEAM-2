{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 쓸모없는 한글자 싹다 날리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yoons\\AppData\\Local\\Temp\\ipykernel_11772\\3384169477.py:24: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == \"\" else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필터링 완료: hawkish_words_del.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def filter_words_from_csv(input_file, output_file):\n",
    "    # CSV 파일 읽기\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # 한 글자 단어 중 유지할 단어 목록\n",
    "    keep_words = {\"하\", \"상\", \"낮\", \"높\", \"크\", \"작\"}\n",
    "    \n",
    "    # 한글 단어 필터링 함수\n",
    "    def filter_text(text):\n",
    "        if not isinstance(text, str):\n",
    "            return text  # 문자열이 아니면 그대로 반환\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if len(word) != 1 or word in keep_words]\n",
    "        return \" \".join(filtered_words)\n",
    "    \n",
    "    # 데이터프레임의 모든 문자열 열에 필터 적용\n",
    "    for col in df.select_dtypes(include=[\"object\"]):\n",
    "        df[col] = df[col].apply(filter_text)\n",
    "    \n",
    "    # 문자열 삭제\n",
    "    df = df.applymap(lambda x: None if x == \"\" else x)\n",
    "    \n",
    "    # 각 셀에서 빈 값 위로 밀기\n",
    "    df = df.apply(lambda col: col.dropna().reset_index(drop=True), axis=0)\n",
    "\n",
    "    # 결과 CSV 저장\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"필터링 완료: {output_file}\")\n",
    "\n",
    "# 사용 예시\n",
    "input_csv = \"hawkish_words.csv\"  # 입력 CSV 파일명\n",
    "output_csv = \"hawkish_words_del.csv\"  # 출력 CSV 파일명\n",
    "filter_words_from_csv(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 중복제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 제거 후 데이터:\n",
      "            Hawkish\n",
      "0                세력\n",
      "1                도발\n",
      "2                철강\n",
      "3      낙관 부동산 가격 상승\n",
      "4                투기\n",
      "...             ...\n",
      "65409            떄문\n",
      "65452            클락\n",
      "65464            식과\n",
      "65465           제티씨\n",
      "65475           양배추\n",
      "\n",
      "[24106 rows x 1 columns]\n",
      "중복 제거된 파일이 hawkish.csv로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. CSV 파일 읽기\n",
    "file_path = 'hawkish_words_del.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 2. 중복 제거하기\n",
    "# 'Dovish' 열에서 중복된 값을 제거 (첫 번째 값만 남김)\n",
    "df_no_duplicates = df.drop_duplicates(subset=['Hawkish'], keep='first')\n",
    "\n",
    "# 3. 결과 확인\n",
    "print(\"중복 제거 후 데이터:\")\n",
    "print(df_no_duplicates)\n",
    "\n",
    "# 4. 새로운 CSV 파일로 저장하기\n",
    "# 저장할 파일 경로 지정. 예: 'data_no_duplicates.csv'\n",
    "output_file_path = 'hawkish.csv'\n",
    "df_no_duplicates.to_csv(output_file_path, index=False, encoding='utf-8-sig')  # 한글 깨짐 방지\n",
    "print(f\"중복 제거된 파일이 {output_file_path}로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 열 이름 확인 ===\n",
      "['Dovish']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'dovish_words_del.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "print()\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
