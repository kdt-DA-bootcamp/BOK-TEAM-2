{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영어 숫자 특수문자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "비둘기파 데이터에서 삭제된 단어 개수: 15\n",
      "매파 데이터에서 삭제된 단어 개수: 24\n",
      "영어, 숫자, 특수문자 포함된 행 제거 및 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 데이터 불러오기\n",
    "df_dovish = pd.read_csv(\"dovish_filtered_no_english.csv\")\n",
    "df_hawkish = pd.read_csv(\"hawkish_filtered_no_english.csv\")\n",
    "\n",
    "# ❗ 컬럼명 직접 지정\n",
    "dovish_column = \"Dovish\"\n",
    "hawkish_column = \"Hawkish\"\n",
    "\n",
    "# 영어, 숫자, 특수문자가 포함된 행을 제거하는 함수\n",
    "def remove_unwanted_rows(df, column_name):\n",
    "    initial_count = len(df)  # 원래 행 개수\n",
    "    df_filtered = df[~df[column_name].astype(str).str.contains(r'[a-zA-Z0-9\\W]', regex=True)]\n",
    "    removed_count = initial_count - len(df_filtered)  # 삭제된 행 개수\n",
    "    return df_filtered, removed_count\n",
    "\n",
    "# 필터링 적용\n",
    "df_dovish, removed_dovish = remove_unwanted_rows(df_dovish, dovish_column)\n",
    "df_hawkish, removed_hawkish = remove_unwanted_rows(df_hawkish, hawkish_column)\n",
    "\n",
    "# 필터링된 데이터 저장\n",
    "df_dovish.to_csv(\"dovish_filtered_clean.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "df_hawkish.to_csv(\"hawkish_filtered_clean.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"비둘기파 데이터에서 삭제된 단어 개수: {removed_dovish}\")\n",
    "print(f\"매파 데이터에서 삭제된 단어 개수: {removed_hawkish}\")\n",
    "print(\"영어, 숫자, 특수문자 포함된 행 제거 및 저장 완료!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 해당 키워드 포함단어 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "비둘기파 데이터에서 '한국' 포함 단어 삭제 개수: 7\n",
      "매파 데이터에서 '한국' 포함 단어 삭제 개수: 0\n",
      "한국 포함 단어 제거 및 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터 불러오기\n",
    "df_dovish = pd.read_csv(\"final_dovish.csv\")\n",
    "df_hawkish = pd.read_csv(\"final_hawkish.csv\")\n",
    "\n",
    "# ❗ 컬럼명 직접 지정\n",
    "dovish_column = \"Dovish\"\n",
    "hawkish_column = \"Hawkish\"\n",
    "\n",
    "# \"한국\"이 포함된 단어를 모두 제거하는 함수\n",
    "def remove_korea_related_words(df, column_name):\n",
    "    initial_count = len(df)  # 원래 행 개수\n",
    "    pattern = r\"홀딩스\"  # '한국'이 포함된 단어를 찾기 위한 패턴\n",
    "    df_filtered = df[~df[column_name].astype(str).str.contains(pattern, regex=True)]\n",
    "    removed_count = initial_count - len(df_filtered)  # 삭제된 행 개수\n",
    "    return df_filtered, removed_count\n",
    "\n",
    "# 필터링 적용\n",
    "df_dovish, removed_dovish = remove_korea_related_words(df_dovish, dovish_column)\n",
    "df_hawkish, removed_hawkish = remove_korea_related_words(df_hawkish, hawkish_column)\n",
    "\n",
    "# 필터링된 데이터 저장\n",
    "df_dovish.to_csv(\"final_dovish.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "df_hawkish.to_csv(\"final_hawkish.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"비둘기파 데이터에서 '한국' 포함 단어 삭제 개수: {removed_dovish}\")\n",
    "print(f\"매파 데이터에서 '한국' 포함 단어 삭제 개수: {removed_hawkish}\")\n",
    "print(\"한국 포함 단어 제거 및 저장 완료!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 필요없는 단어들 때문에 순도 높은 사전 구축이 되질 않는다\n",
    "사전을 보고 필요없는 단어를 계속 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF 계산 ( 매파 비둘기 파 잘 구분하는가?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TF-IDF 계산 완료! 결과가 tfidf_hawk_dove.csv 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 🔹 CSV 파일 불러오기\n",
    "df_dovish = pd.read_csv(\"final_dovish.csv\")  # 비둘기파\n",
    "df_hawkish = pd.read_csv(\"final_hawkish.csv\")  # 매파\n",
    "\n",
    "# 🔹 단어들을 하나의 문서처럼 결합\n",
    "dovish_text = ' '.join(df_dovish[\"Dovish\"])\n",
    "hawkish_text = ' '.join(df_hawkish[\"Hawkish\"])\n",
    "\n",
    "# 🔹 문서 리스트 생성 (비둘기파, 매파)\n",
    "documents = [dovish_text, hawkish_text]\n",
    "\n",
    "# 🔹 TF-IDF 벡터라이저 적용\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# 🔹 단어별 TF-IDF 점수 추출\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tfidf_scores = tfidf_matrix.toarray()\n",
    "\n",
    "# 🔹 데이터프레임 변환\n",
    "tfidf_df = pd.DataFrame(tfidf_scores.T, index=words, columns=[\"비둘기파\", \"매파\"])\n",
    "\n",
    "# 🔹 CSV로 저장\n",
    "tfidf_df.to_csv(\"Fast-taxt_TF-IDF.csv\")\n",
    "\n",
    "print(\"파일 저장\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Dovish', 'Category'], dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dovish.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Dovish Hawkish\n",
      "0        하향      세력\n",
      "1        합의      도발\n",
      "2        부진      철강\n",
      "3      상호이해      투기\n",
      "4        평화       높\n",
      "...     ...     ...\n",
      "5963    NaN      수고\n",
      "5964    NaN     셰브론\n",
      "5965    NaN      처전\n",
      "5966    NaN      딴판\n",
      "5967    NaN      펜딩\n",
      "\n",
      "[5968 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 두 CSV 파일 읽기\n",
    "df_dovish = pd.read_csv(\"final_dovish.csv\")\n",
    "df_hawkish = pd.read_csv(\"final_hawkish.csv\")\n",
    "\n",
    "# 각 데이터프레임의 'Dovish' 열과 'Hawkish' 열만 추출\n",
    "dovish_words = df_dovish['Dovish']\n",
    "hawkish_words = df_hawkish['Hawkish']\n",
    "\n",
    "# 길이가 맞지 않다면 더 짧은 열을 맞추기 위해 NaN으로 채움\n",
    "max_len = max(len(dovish_words), len(hawkish_words))\n",
    "dovish_words = dovish_words.reindex(range(max_len))\n",
    "hawkish_words = hawkish_words.reindex(range(max_len))\n",
    "\n",
    "# 두 열을 하나의 데이터프레임으로 결합\n",
    "combined_df = pd.DataFrame({\n",
    "    'Dovish': dovish_words,\n",
    "    'Hawkish': hawkish_words\n",
    "})\n",
    "\n",
    "# 결과 확인\n",
    "print(combined_df)\n",
    "\n",
    "# 결과를 새로운 CSV로 저장\n",
    "combined_df.to_csv(\"combined_keywords.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 같은 크기로 맞추기로 맞춰서 두 사전간 유사도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "Common Columns: {'dovish', 'hawkish'}\n",
      "Comparing column: dovish\n",
      "Set1 size: 5586, Set2 size: 5586\n",
      "Set1 example: ['준편차', '시장조사', '충당금적립전', '사옥', '유니버설']\n",
      "Set2 example: ['호주 한국', '철강 생산능력', '환율 시장 개입', '전세 사기 사건', '전월 비 상승 폭']\n",
      "Comparing column: hawkish\n",
      "Set1 size: 5586, Set2 size: 5586\n",
      "Set1 example: ['내정자', '케프투자증권', '누그러질', '계양', '트래커']\n",
      "Set2 example: ['압력 시각', '하락 마땅 투자처', '개발 아파트', '지난해 업계', '운수 상하이 인터내셔널 포트']\n",
      "Column: dovish, Jaccard Similarity: 0.004044217\n",
      "Column: hawkish, Jaccard Similarity: 0.004766616\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 파일 경로 확인\n",
    "print(os.path.exists('Fast-text_final.csv'))\n",
    "print(os.path.exists('예진.csv'))\n",
    "\n",
    "# 두 파일을 문자열 타입으로 읽어들임\n",
    "df1 = pd.read_csv('Fast-text_final.csv', dtype=str)\n",
    "df2 = pd.read_csv('예진.csv', dtype=str)\n",
    "\n",
    "# 둘 중 작은 크기로 맞추기\n",
    "min_length = min(len(df1), len(df2))\n",
    "\n",
    "# 두 데이터프레임의 길이를 같게 자르기\n",
    "df1 = df1.head(min_length)\n",
    "df2 = df2.head(min_length)\n",
    "\n",
    "# 컬럼명을 모두 소문자로 변환하여 일관성 맞추기\n",
    "df1.columns = df1.columns.str.lower()  # df1의 컬럼명을 소문자로 변환\n",
    "df2.columns = df2.columns.str.lower()  # df2의 컬럼명을 소문자로 변환\n",
    "\n",
    "# 두 DataFrame에 공통으로 있는 컬럼만 비교\n",
    "common_columns = set(df1.columns).intersection(set(df2.columns))\n",
    "print(\"Common Columns:\", common_columns)\n",
    "\n",
    "# 자카드 유사도 계산\n",
    "similarity_results = {}\n",
    "\n",
    "for col in common_columns:\n",
    "    # 각 컬럼의 값들을 집합으로 변환 (NaN 제거, 좌우 공백 제거)\n",
    "    set1 = set(df1[col].dropna().str.strip())\n",
    "    set2 = set(df2[col].dropna().str.strip())\n",
    "    \n",
    "    # 유사도 계산 전, 두 집합에 포함된 값 확인\n",
    "    print(f\"Comparing column: {col}\")\n",
    "    print(f\"Set1 size: {len(set1)}, Set2 size: {len(set2)}\")\n",
    "    print(f\"Set1 example: {list(set1)[:5]}\")\n",
    "    print(f\"Set2 example: {list(set2)[:5]}\")\n",
    "    \n",
    "    # 교집합과 합집합 계산\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    \n",
    "    # 자카드 유사도: (교집합의 크기) / (합집합의 크기)\n",
    "    jaccard_similarity = len(intersection) / len(union) if union else 0\n",
    "    similarity_results[col] = jaccard_similarity\n",
    "\n",
    "# 결과 출력\n",
    "if not similarity_results:\n",
    "    print(\"No similarity results found.\")\n",
    "else:\n",
    "    for col, sim in similarity_results.items():\n",
    "        print(f\"Column: {col}, Jaccard Similarity: {sim:.9f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
