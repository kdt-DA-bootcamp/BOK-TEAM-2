{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì˜ì–´ ìˆ«ì íŠ¹ìˆ˜ë¬¸ì ì œê±°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¹„ë‘˜ê¸°íŒŒ ë°ì´í„°ì—ì„œ ì‚­ì œëœ ë‹¨ì–´ ê°œìˆ˜: 15\n",
      "ë§¤íŒŒ ë°ì´í„°ì—ì„œ ì‚­ì œëœ ë‹¨ì–´ ê°œìˆ˜: 24\n",
      "ì˜ì–´, ìˆ«ì, íŠ¹ìˆ˜ë¬¸ì í¬í•¨ëœ í–‰ ì œê±° ë° ì €ì¥ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df_dovish = pd.read_csv(\"dovish_filtered_no_english.csv\")\n",
    "df_hawkish = pd.read_csv(\"hawkish_filtered_no_english.csv\")\n",
    "\n",
    "# â— ì»¬ëŸ¼ëª… ì§ì ‘ ì§€ì •\n",
    "dovish_column = \"Dovish\"\n",
    "hawkish_column = \"Hawkish\"\n",
    "\n",
    "# ì˜ì–´, ìˆ«ì, íŠ¹ìˆ˜ë¬¸ìê°€ í¬í•¨ëœ í–‰ì„ ì œê±°í•˜ëŠ” í•¨ìˆ˜\n",
    "def remove_unwanted_rows(df, column_name):\n",
    "    initial_count = len(df)  # ì›ë˜ í–‰ ê°œìˆ˜\n",
    "    df_filtered = df[~df[column_name].astype(str).str.contains(r'[a-zA-Z0-9\\W]', regex=True)]\n",
    "    removed_count = initial_count - len(df_filtered)  # ì‚­ì œëœ í–‰ ê°œìˆ˜\n",
    "    return df_filtered, removed_count\n",
    "\n",
    "# í•„í„°ë§ ì ìš©\n",
    "df_dovish, removed_dovish = remove_unwanted_rows(df_dovish, dovish_column)\n",
    "df_hawkish, removed_hawkish = remove_unwanted_rows(df_hawkish, hawkish_column)\n",
    "\n",
    "# í•„í„°ë§ëœ ë°ì´í„° ì €ì¥\n",
    "df_dovish.to_csv(\"dovish_filtered_clean.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "df_hawkish.to_csv(\"hawkish_filtered_clean.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"ë¹„ë‘˜ê¸°íŒŒ ë°ì´í„°ì—ì„œ ì‚­ì œëœ ë‹¨ì–´ ê°œìˆ˜: {removed_dovish}\")\n",
    "print(f\"ë§¤íŒŒ ë°ì´í„°ì—ì„œ ì‚­ì œëœ ë‹¨ì–´ ê°œìˆ˜: {removed_hawkish}\")\n",
    "print(\"ì˜ì–´, ìˆ«ì, íŠ¹ìˆ˜ë¬¸ì í¬í•¨ëœ í–‰ ì œê±° ë° ì €ì¥ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í•´ë‹¹ í‚¤ì›Œë“œ í¬í•¨ë‹¨ì–´ ì‚­ì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¹„ë‘˜ê¸°íŒŒ ë°ì´í„°ì—ì„œ 'í•œêµ­' í¬í•¨ ë‹¨ì–´ ì‚­ì œ ê°œìˆ˜: 7\n",
      "ë§¤íŒŒ ë°ì´í„°ì—ì„œ 'í•œêµ­' í¬í•¨ ë‹¨ì–´ ì‚­ì œ ê°œìˆ˜: 0\n",
      "í•œêµ­ í¬í•¨ ë‹¨ì–´ ì œê±° ë° ì €ì¥ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df_dovish = pd.read_csv(\"final_dovish.csv\")\n",
    "df_hawkish = pd.read_csv(\"final_hawkish.csv\")\n",
    "\n",
    "# â— ì»¬ëŸ¼ëª… ì§ì ‘ ì§€ì •\n",
    "dovish_column = \"Dovish\"\n",
    "hawkish_column = \"Hawkish\"\n",
    "\n",
    "# \"í•œêµ­\"ì´ í¬í•¨ëœ ë‹¨ì–´ë¥¼ ëª¨ë‘ ì œê±°í•˜ëŠ” í•¨ìˆ˜\n",
    "def remove_korea_related_words(df, column_name):\n",
    "    initial_count = len(df)  # ì›ë˜ í–‰ ê°œìˆ˜\n",
    "    pattern = r\"í™€ë”©ìŠ¤\"  # 'í•œêµ­'ì´ í¬í•¨ëœ ë‹¨ì–´ë¥¼ ì°¾ê¸° ìœ„í•œ íŒ¨í„´\n",
    "    df_filtered = df[~df[column_name].astype(str).str.contains(pattern, regex=True)]\n",
    "    removed_count = initial_count - len(df_filtered)  # ì‚­ì œëœ í–‰ ê°œìˆ˜\n",
    "    return df_filtered, removed_count\n",
    "\n",
    "# í•„í„°ë§ ì ìš©\n",
    "df_dovish, removed_dovish = remove_korea_related_words(df_dovish, dovish_column)\n",
    "df_hawkish, removed_hawkish = remove_korea_related_words(df_hawkish, hawkish_column)\n",
    "\n",
    "# í•„í„°ë§ëœ ë°ì´í„° ì €ì¥\n",
    "df_dovish.to_csv(\"final_dovish.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "df_hawkish.to_csv(\"final_hawkish.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"ë¹„ë‘˜ê¸°íŒŒ ë°ì´í„°ì—ì„œ 'í•œêµ­' í¬í•¨ ë‹¨ì–´ ì‚­ì œ ê°œìˆ˜: {removed_dovish}\")\n",
    "print(f\"ë§¤íŒŒ ë°ì´í„°ì—ì„œ 'í•œêµ­' í¬í•¨ ë‹¨ì–´ ì‚­ì œ ê°œìˆ˜: {removed_hawkish}\")\n",
    "print(\"í•œêµ­ í¬í•¨ ë‹¨ì–´ ì œê±° ë° ì €ì¥ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### í•„ìš”ì—†ëŠ” ë‹¨ì–´ë“¤ ë•Œë¬¸ì— ìˆœë„ ë†’ì€ ì‚¬ì „ êµ¬ì¶•ì´ ë˜ì§ˆ ì•ŠëŠ”ë‹¤\n",
    "ì‚¬ì „ì„ ë³´ê³  í•„ìš”ì—†ëŠ” ë‹¨ì–´ë¥¼ ê³„ì† ì œê±°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF ê³„ì‚° ( ë§¤íŒŒ ë¹„ë‘˜ê¸° íŒŒ ì˜ êµ¬ë¶„í•˜ëŠ”ê°€?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TF-IDF ê³„ì‚° ì™„ë£Œ! ê²°ê³¼ê°€ tfidf_hawk_dove.csv íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ğŸ”¹ CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df_dovish = pd.read_csv(\"final_dovish.csv\")  # ë¹„ë‘˜ê¸°íŒŒ\n",
    "df_hawkish = pd.read_csv(\"final_hawkish.csv\")  # ë§¤íŒŒ\n",
    "\n",
    "# ğŸ”¹ ë‹¨ì–´ë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ì„œì²˜ëŸ¼ ê²°í•©\n",
    "dovish_text = ' '.join(df_dovish[\"Dovish\"])\n",
    "hawkish_text = ' '.join(df_hawkish[\"Hawkish\"])\n",
    "\n",
    "# ğŸ”¹ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ìƒì„± (ë¹„ë‘˜ê¸°íŒŒ, ë§¤íŒŒ)\n",
    "documents = [dovish_text, hawkish_text]\n",
    "\n",
    "# ğŸ”¹ TF-IDF ë²¡í„°ë¼ì´ì € ì ìš©\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# ğŸ”¹ ë‹¨ì–´ë³„ TF-IDF ì ìˆ˜ ì¶”ì¶œ\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tfidf_scores = tfidf_matrix.toarray()\n",
    "\n",
    "# ğŸ”¹ ë°ì´í„°í”„ë ˆì„ ë³€í™˜\n",
    "tfidf_df = pd.DataFrame(tfidf_scores.T, index=words, columns=[\"ë¹„ë‘˜ê¸°íŒŒ\", \"ë§¤íŒŒ\"])\n",
    "\n",
    "# ğŸ”¹ CSVë¡œ ì €ì¥\n",
    "tfidf_df.to_csv(\"Fast-taxt_TF-IDF.csv\")\n",
    "\n",
    "print(\"íŒŒì¼ ì €ì¥\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Dovish', 'Category'], dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dovish.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Dovish Hawkish\n",
      "0        í•˜í–¥      ì„¸ë ¥\n",
      "1        í•©ì˜      ë„ë°œ\n",
      "2        ë¶€ì§„      ì² ê°•\n",
      "3      ìƒí˜¸ì´í•´      íˆ¬ê¸°\n",
      "4        í‰í™”       ë†’\n",
      "...     ...     ...\n",
      "5963    NaN      ìˆ˜ê³ \n",
      "5964    NaN     ì…°ë¸Œë¡ \n",
      "5965    NaN      ì²˜ì „\n",
      "5966    NaN      ë”´íŒ\n",
      "5967    NaN      íœë”©\n",
      "\n",
      "[5968 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ë‘ CSV íŒŒì¼ ì½ê¸°\n",
    "df_dovish = pd.read_csv(\"final_dovish.csv\")\n",
    "df_hawkish = pd.read_csv(\"final_hawkish.csv\")\n",
    "\n",
    "# ê° ë°ì´í„°í”„ë ˆì„ì˜ 'Dovish' ì—´ê³¼ 'Hawkish' ì—´ë§Œ ì¶”ì¶œ\n",
    "dovish_words = df_dovish['Dovish']\n",
    "hawkish_words = df_hawkish['Hawkish']\n",
    "\n",
    "# ê¸¸ì´ê°€ ë§ì§€ ì•Šë‹¤ë©´ ë” ì§§ì€ ì—´ì„ ë§ì¶”ê¸° ìœ„í•´ NaNìœ¼ë¡œ ì±„ì›€\n",
    "max_len = max(len(dovish_words), len(hawkish_words))\n",
    "dovish_words = dovish_words.reindex(range(max_len))\n",
    "hawkish_words = hawkish_words.reindex(range(max_len))\n",
    "\n",
    "# ë‘ ì—´ì„ í•˜ë‚˜ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ê²°í•©\n",
    "combined_df = pd.DataFrame({\n",
    "    'Dovish': dovish_words,\n",
    "    'Hawkish': hawkish_words\n",
    "})\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(combined_df)\n",
    "\n",
    "# ê²°ê³¼ë¥¼ ìƒˆë¡œìš´ CSVë¡œ ì €ì¥\n",
    "combined_df.to_csv(\"combined_keywords.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê°™ì€ í¬ê¸°ë¡œ ë§ì¶”ê¸°ë¡œ ë§ì¶°ì„œ ë‘ ì‚¬ì „ê°„ ìœ ì‚¬ë„ ê³„ì‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "Common Columns: {'dovish', 'hawkish'}\n",
      "Comparing column: dovish\n",
      "Set1 size: 5586, Set2 size: 5586\n",
      "Set1 example: ['ì¤€í¸ì°¨', 'ì‹œì¥ì¡°ì‚¬', 'ì¶©ë‹¹ê¸ˆì ë¦½ì „', 'ì‚¬ì˜¥', 'ìœ ë‹ˆë²„ì„¤']\n",
      "Set2 example: ['í˜¸ì£¼ í•œêµ­', 'ì² ê°• ìƒì‚°ëŠ¥ë ¥', 'í™˜ìœ¨ ì‹œì¥ ê°œì…', 'ì „ì„¸ ì‚¬ê¸° ì‚¬ê±´', 'ì „ì›” ë¹„ ìƒìŠ¹ í­']\n",
      "Comparing column: hawkish\n",
      "Set1 size: 5586, Set2 size: 5586\n",
      "Set1 example: ['ë‚´ì •ì', 'ì¼€í”„íˆ¬ìì¦ê¶Œ', 'ëˆ„ê·¸ëŸ¬ì§ˆ', 'ê³„ì–‘', 'íŠ¸ë˜ì»¤']\n",
      "Set2 example: ['ì••ë ¥ ì‹œê°', 'í•˜ë½ ë§ˆë•… íˆ¬ìì²˜', 'ê°œë°œ ì•„íŒŒíŠ¸', 'ì§€ë‚œí•´ ì—…ê³„', 'ìš´ìˆ˜ ìƒí•˜ì´ ì¸í„°ë‚´ì…”ë„ í¬íŠ¸']\n",
      "Column: dovish, Jaccard Similarity: 0.004044217\n",
      "Column: hawkish, Jaccard Similarity: 0.004766616\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ í™•ì¸\n",
    "print(os.path.exists('Fast-text_final.csv'))\n",
    "print(os.path.exists('ì˜ˆì§„.csv'))\n",
    "\n",
    "# ë‘ íŒŒì¼ì„ ë¬¸ìì—´ íƒ€ì…ìœ¼ë¡œ ì½ì–´ë“¤ì„\n",
    "df1 = pd.read_csv('Fast-text_final.csv', dtype=str)\n",
    "df2 = pd.read_csv('ì˜ˆì§„.csv', dtype=str)\n",
    "\n",
    "# ë‘˜ ì¤‘ ì‘ì€ í¬ê¸°ë¡œ ë§ì¶”ê¸°\n",
    "min_length = min(len(df1), len(df2))\n",
    "\n",
    "# ë‘ ë°ì´í„°í”„ë ˆì„ì˜ ê¸¸ì´ë¥¼ ê°™ê²Œ ìë¥´ê¸°\n",
    "df1 = df1.head(min_length)\n",
    "df2 = df2.head(min_length)\n",
    "\n",
    "# ì»¬ëŸ¼ëª…ì„ ëª¨ë‘ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ì¼ê´€ì„± ë§ì¶”ê¸°\n",
    "df1.columns = df1.columns.str.lower()  # df1ì˜ ì»¬ëŸ¼ëª…ì„ ì†Œë¬¸ìë¡œ ë³€í™˜\n",
    "df2.columns = df2.columns.str.lower()  # df2ì˜ ì»¬ëŸ¼ëª…ì„ ì†Œë¬¸ìë¡œ ë³€í™˜\n",
    "\n",
    "# ë‘ DataFrameì— ê³µí†µìœ¼ë¡œ ìˆëŠ” ì»¬ëŸ¼ë§Œ ë¹„êµ\n",
    "common_columns = set(df1.columns).intersection(set(df2.columns))\n",
    "print(\"Common Columns:\", common_columns)\n",
    "\n",
    "# ìì¹´ë“œ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "similarity_results = {}\n",
    "\n",
    "for col in common_columns:\n",
    "    # ê° ì»¬ëŸ¼ì˜ ê°’ë“¤ì„ ì§‘í•©ìœ¼ë¡œ ë³€í™˜ (NaN ì œê±°, ì¢Œìš° ê³µë°± ì œê±°)\n",
    "    set1 = set(df1[col].dropna().str.strip())\n",
    "    set2 = set(df2[col].dropna().str.strip())\n",
    "    \n",
    "    # ìœ ì‚¬ë„ ê³„ì‚° ì „, ë‘ ì§‘í•©ì— í¬í•¨ëœ ê°’ í™•ì¸\n",
    "    print(f\"Comparing column: {col}\")\n",
    "    print(f\"Set1 size: {len(set1)}, Set2 size: {len(set2)}\")\n",
    "    print(f\"Set1 example: {list(set1)[:5]}\")\n",
    "    print(f\"Set2 example: {list(set2)[:5]}\")\n",
    "    \n",
    "    # êµì§‘í•©ê³¼ í•©ì§‘í•© ê³„ì‚°\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    \n",
    "    # ìì¹´ë“œ ìœ ì‚¬ë„: (êµì§‘í•©ì˜ í¬ê¸°) / (í•©ì§‘í•©ì˜ í¬ê¸°)\n",
    "    jaccard_similarity = len(intersection) / len(union) if union else 0\n",
    "    similarity_results[col] = jaccard_similarity\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "if not similarity_results:\n",
    "    print(\"No similarity results found.\")\n",
    "else:\n",
    "    for col, sim in similarity_results.items():\n",
    "        print(f\"Column: {col}, Jaccard Similarity: {sim:.9f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
