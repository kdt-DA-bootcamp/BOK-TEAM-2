{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NBC 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('labeled_economy.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ngram_list(x):\n",
    "    return ast.literal_eval(x)\n",
    "\n",
    "df['ngram_list'] = df['ngram_label'].apply(parse_ngram_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_and_target(ngram_list):\n",
    "    if not ngram_list:\n",
    "        return \"\", None\n",
    "    \n",
    "    # 초기화\n",
    "    tokens = []\n",
    "    target = None\n",
    "\n",
    "    for item in ngram_list:\n",
    "        token_tuple, freq, lab = item\n",
    "        # item이 (('금리',), 13, 0) 형태일 때,\n",
    "        # item[0] = 토큰 튜플\n",
    "        # item[1] = 빈도수(freq)\n",
    "        # item[2] = 라벨(0이면 dovish, 1이면 hawkish)\n",
    "\n",
    "        # 튜플을 문자열로 만들어 합친다.\n",
    "        token_str = \"_\".join(token_tuple)\n",
    "        if token_str not in tokens:\n",
    "            tokens.append(token_str)\n",
    "\n",
    "        if lab == 1:\n",
    "            target = \"hawkish\"\n",
    "        elif lab == -1:\n",
    "            target = \"dovish\"\n",
    "\n",
    "    document = \" \".join(tokens) if tokens else \"\"\n",
    "    return document, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['document', 'target']] = df['ngram_list'].apply(lambda x: pd.Series(create_document_and_target(x)))\n",
    "df = df.dropna(subset=['document', 'target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      dovish       0.64      1.00      0.78        16\n",
      "     hawkish       1.00      0.50      0.67        18\n",
      "\n",
      "    accuracy                           0.74        34\n",
      "   macro avg       0.82      0.75      0.72        34\n",
      "weighted avg       0.83      0.74      0.72        34\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 피처(X)와 타겟(y) 준비\n",
    "X = df['document']\n",
    "y = df['target']\n",
    "\n",
    "# 학습/테스트 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# CountVectorizer로 문서 벡터화\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# 나이브 베이즈 분류기(MultinomialNB) 학습\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_vec, y_train)\n",
    "\n",
    "# 테스트 데이터 예측 및 평가\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스 순서: ['dovish' 'hawkish']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "probs = np.exp(clf.feature_log_prob_)\n",
    "print(\"클래스 순서:\", clf.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hawkish 쪽 상위 단어:\n",
      "     feature  dovish_prob  hawkish_prob  sentiment_score\n",
      "1948      인상     0.002567      0.006239         0.003673\n",
      "498    금리_인상     0.001497      0.004771         0.003274\n",
      "915       물가     0.004919      0.007707         0.002788\n",
      "703       대비     0.004385      0.007065         0.002680\n",
      "26       fed     0.004919      0.007432         0.002513\n",
      "1423     소비자     0.002887      0.005230         0.002342\n",
      "1969   인플레이션     0.001818      0.004129         0.002311\n",
      "612       긴축     0.000321      0.002569         0.002248\n",
      "2206      전치     0.000107      0.002110         0.002003\n",
      "1643      압력     0.001497      0.003395         0.001898\n",
      "\n",
      "Dovish 쪽 상위 단어:\n",
      "     feature  dovish_prob  hawkish_prob  sentiment_score\n",
      "2394      중국     0.010908      0.006606        -0.004302\n",
      "1975      인하     0.004278      0.001285        -0.002993\n",
      "507    금리_인하     0.003529      0.000734        -0.002795\n",
      "236       경제     0.008555      0.005964        -0.002591\n",
      "937       미국     0.012512      0.010184        -0.002328\n",
      "898       무역     0.003957      0.001835        -0.002122\n",
      "2880      하락     0.007700      0.005689        -0.002011\n",
      "1544      시장     0.004919      0.002936        -0.001983\n",
      "1994      일본     0.003850      0.002110        -0.001740\n",
      "2773     트럼프     0.001818      0.000184        -0.001634\n"
     ]
    }
   ],
   "source": [
    "sentiment_score = probs[1] - probs[0]\n",
    "\n",
    "sentiment_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'dovish_prob': probs[0],\n",
    "    'hawkish_prob': probs[1],\n",
    "    'sentiment_score': sentiment_score\n",
    "})\n",
    "\n",
    "print(\"Hawkish 쪽 상위 단어:\")\n",
    "print(sentiment_df.sort_values('sentiment_score', ascending=False).head(10))\n",
    "print(\"\\nDovish 쪽 상위 단어:\")\n",
    "print(sentiment_df.sort_values('sentiment_score', ascending=True).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target 별로 모든 document를 하나의 텍스트로 합치기\n",
    "grouped_docs = df.groupby('target')['document'].apply(lambda docs: \" \".join(docs)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dovish : 한국 미국 제조업 경기 수출 불확실성 상승 지수 SP 생산 감소 추세 지속 정부 기업 정책 기대 내년 고용 서비스업 임금 상승률 둔화 확대 발표 ISM PMI 흐름 필요 국내 트럼프 수입 중국 증가 크 우려 PCE 확인 소비 연말 판단 가운데 매크로 이벤트 비농업 신규 실업률 주요 유로존 기준 하락 속도 경제 독일 금리 인하 관련 인플레이션 압력 서비스 소...\n",
      "hawkish : 내용 작성 확인 투자 판단 증권 물가 상승 미국 인플레이션 높 상승률 평균 경제 수요 한국 유지 지속 발표 국가 이벤트 대비 분기 전월 에너지 제외 비 수입 지수 소매 판매 가스 생산 주택 매크로 소매_판매 고용 소비자 fed 시장 가격 가중치 소비 데이터 기준 비중 서비스 하락 증가 주거비 크 주요 금리 인상 둔화 경기 유로존 소비자_물가 금리_인상 팀 ...\n"
     ]
    }
   ],
   "source": [
    "for idx, row in grouped_docs.iterrows():\n",
    "    print(f\"{row['target']} : {row['document'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
