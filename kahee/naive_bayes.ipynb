{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading CSV: 1it [03:24, 204.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ë¶„ë¦¬ ì™„ë£Œ! train_data.csv & test_data.csv ìƒì„±ë¨\n"
     ]
    }
   ],
   "source": [
    "# ëŒ€ìš©ëŸ‰ CSV íŒŒì¼ ê²½ë¡œ\n",
    "input_csv = r'C:\\Users\\bona_\\infomax_crawler\\labeled_edaily.csv'\n",
    "train_csv = \"train_data.csv\"\n",
    "test_csv = \"test_data.csv\"\n",
    "\n",
    "# ì²­í¬ í¬ê¸° ì„¤ì •\n",
    "chunksize = 5000  # ì¡°ì • ê°€ëŠ¥\n",
    "\n",
    "def sentiment_label(x):\n",
    "    return ast.literal_eval(x)\n",
    "\n",
    "def make_model(nbc):\n",
    "    if not nbc:\n",
    "        return \"\", None\n",
    "    \n",
    "    tokens = []\n",
    "    target = None\n",
    "\n",
    "    for item in nbc:\n",
    "        token, freq, label = item\n",
    "\n",
    "        token_str = \"_\".join(token)\n",
    "        if token_str not in tokens:\n",
    "            tokens.append(token_str)\n",
    "\n",
    "        if label == 1:\n",
    "            target = \"hawkish\"\n",
    "        elif label == -1:\n",
    "            target = \"dovish\"\n",
    "\n",
    "    document = \" \".join(tokens) if tokens else \"\"\n",
    "    return document, target\n",
    "\n",
    "# ì „ì²´ ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "document_list = []\n",
    "target_list = []\n",
    "\n",
    "# CSV íŒŒì¼ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ì½ê¸°\n",
    "for chunk in tqdm(pd.read_csv(input_csv, chunksize=chunksize), desc=\"Reading CSV\"):\n",
    "    # ngram_label ì»¬ëŸ¼ì„ íŒŒì‹±í•˜ì—¬ nbc ì»¬ëŸ¼ ìƒì„±\n",
    "    chunk[\"nbc\"] = chunk[\"ngram_label\"].apply(sentiment_label)\n",
    "\n",
    "    # make_model ì ìš© í›„ document, target ì»¬ëŸ¼ ìƒì„±\n",
    "    chunk[[\"document\", \"target\"]] = chunk[\"nbc\"].apply(lambda x: pd.Series(make_model(x)))\n",
    "\n",
    "    # ë¹ˆ ë¬¸ìì—´ & ì¤‘ë¦½(\"neutral\") ê°’ ì œê±°\n",
    "    chunk = chunk[(chunk[\"document\"] != \"\") & (chunk[\"target\"].notna())]\n",
    "\n",
    "    # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "    document_list.extend(chunk[\"document\"].tolist())\n",
    "    target_list.extend(chunk[\"target\"].tolist())\n",
    "\n",
    "# ğŸ”¹ ì „ì²´ ë°ì´í„° í”„ë ˆì„ ìƒì„±\n",
    "full_data = pd.DataFrame({\"document\": document_list, \"target\": target_list})\n",
    "\n",
    "# ğŸ”¹ ë°ì´í„°ê°€ ì¶©ë¶„í•œì§€ í™•ì¸\n",
    "if len(full_data) < 2:\n",
    "    print(\"ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ train_test_splitì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    # ğŸ”¹ í´ë˜ìŠ¤ê°€ í•˜ë‚˜ë¿ì´ë©´ stratify ì‚¬ìš© X\n",
    "    class_counts = full_data[\"target\"].value_counts()\n",
    "    stratify_param = full_data[\"target\"] if len(class_counts) > 1 else None\n",
    "\n",
    "    # ğŸ”¹ train-test split ì‹¤í–‰\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        full_data[\"document\"], full_data[\"target\"], test_size=0.1, random_state=42, stratify=stratify_param\n",
    "    )\n",
    "\n",
    "    # ğŸ”¹ DataFrame ë³€í™˜\n",
    "    train_data = pd.DataFrame({\"document\": X_train, \"target\": y_train})\n",
    "    test_data = pd.DataFrame({\"document\": X_test, \"target\": y_test})\n",
    "\n",
    "    # ğŸ”¹ CSV íŒŒì¼ ì €ì¥\n",
    "    train_data.to_csv(train_csv, index=False)\n",
    "    test_data.to_csv(test_csv, index=False)\n",
    "\n",
    "    print(\"ë°ì´í„° ë¶„ë¦¬ ì™„ë£Œ! train_data.csv & test_data.csv ìƒì„±ë¨\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì €ì¥ëœ train/test ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "train_data = pd.read_csv(\"train_data.csv\")\n",
    "test_data = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "X_train = train_data[\"document\"].fillna(\"\")  # ê²°ì¸¡ê°’ ë°©ì§€\n",
    "y_train = train_data[\"target\"].fillna(\"neutral\")  # ê²°ì¸¡ê°’ ë°©ì§€\n",
    "X_test = test_data[\"document\"].fillna(\"\")\n",
    "y_test = test_data[\"target\"].fillna(\"neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_set TF-IDF ë²¡í„°í™”: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 510/510 [00:04<00:00, 122.79it/s]\n",
      "Test_set TF-IDF ë²¡í„°í™” ì§„í–‰: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 154.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# TfidfVectorizerë¡œ ë¬¸ì„œ ë²¡í„°í™”\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,5), max_features=5000)\n",
    "X_train_vec = vectorizer.fit_transform(tqdm(X_train, desc=\"Train_set TF-IDF ë²¡í„°í™”\"))\n",
    "X_test_vec = vectorizer.transform(tqdm(X_test, desc=\"Test_set TF-IDF ë²¡í„°í™” ì§„í–‰\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.pkl']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ í•™ìŠµ(ìµœì´ˆ)\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_vec, y_train)\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥(ì´í›„ ê²°ê³¼ ëˆ„ì  ìœ„í•´)\n",
    "import joblib\n",
    "joblib.dump(nb, \"naive_bayes_model.pkl\")\n",
    "joblib.dump(vectorizer, \"vectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###ëˆ„ì í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê¸°ì¡´ ëª¨ë¸ ë° ë²¡í„°ë¼ì´ì € ë¡œë“œ ì™„ë£Œ\n",
      "ngram_label => document, targetìœ¼ë¡œ ë³€í™˜ ì¤‘\n",
      "ìƒˆë¡œìš´ ë°ì´í„° ê°œìˆ˜: 410\n",
      "partial_fit ì§€ì›í•¨. ì¶”ê°€ í•™ìŠµ ì§„í–‰\n",
      "ëª¨ë¸ íŒŒì¼ í¬ê¸°: 160839\n",
      "ëª¨ë¸ ì—…ë°ì´íŠ¸!\n"
     ]
    }
   ],
   "source": [
    "# ê¸°ì¡´ ëª¨ë¸ ë° ë²¡í„°ë¼ì´ì € ë¡œë“œ\n",
    "try:\n",
    "    nb = joblib.load(\"naive_bayes_model.pkl\")\n",
    "    vectorizer = joblib.load(\"vectorizer.pkl\")\n",
    "    print(\"ê¸°ì¡´ ëª¨ë¸ ë° ë²¡í„°ë¼ì´ì € ë¡œë“œ ì™„ë£Œ\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ê¸°ì¡´ ëª¨ë¸ì´ ì—†ìŒ\")\n",
    "    exit()\n",
    "\n",
    "# ìƒˆë¡œìš´ CSV íŒŒì¼\n",
    "new_news_csv = r'C:\\Users\\bona_\\infomax_crawler\\labeled_economy.csv'\n",
    "new_df = pd.read_csv(new_news_csv)\n",
    "\n",
    "# `make_model()`ì„ ì ìš©í•˜ì—¬ 'ngram' -> `document`, `target` ìƒì„±\n",
    "if \"document\" not in new_df.columns:\n",
    "    if \"ngram_label\" in new_df.columns:\n",
    "        print(\"ngram_label => document, targetìœ¼ë¡œ ë³€í™˜ ì¤‘\")\n",
    "        new_df[[\"document\", \"target\"]] = new_df[\"ngram_label\"].apply(lambda x: pd.Series(make_model(ast.literal_eval(x))))\n",
    "    else:\n",
    "        print(\"'document', 'ngram_label' ì—†ìŒ\")\n",
    "        exit()\n",
    "\n",
    "# ë°ì´í„° í™•ì¸ (ì¤‘ë³µ ë°ì´í„° í™•ì¸)\n",
    "new_df = new_df[(new_df[\"document\"] != \"\") & (new_df[\"target\"].notna())]\n",
    "print(\"ìƒˆë¡œìš´ ë°ì´í„° ê°œìˆ˜:\", len(new_df))\n",
    "\n",
    "if len(new_df) == 0:\n",
    "    print(\"ìƒˆë¡œìš´ ë°ì´í„° ì¡´ì¬X, í•™ìŠµ ì¢…ë£Œë£Œ\")\n",
    "    exit()\n",
    "\n",
    "# TF-IDF ë³€í™˜ (ê¸°ì¡´ ë²¡í„°ë¼ì´ì € ì‚¬ìš©)\n",
    "X_new = vectorizer.transform(new_df[\"document\"].fillna(\"\"))\n",
    "y_new = new_df[\"target\"].astype(str)\n",
    "\n",
    "# partial_fit ì§€ì›ì—¬ë¶€ í™•ì¸ì¸\n",
    "if hasattr(nb, \"partial_fit\"):\n",
    "    print(\"partial_fit ì§€ì›í•¨. ì¶”ê°€ í•™ìŠµ ì§„í–‰\")\n",
    "    nb.partial_fit(X_new, y_new, classes=[\"hawkish\", \"dovish\"])\n",
    "else:\n",
    "    print(\"partial_fit ì§€ì› x. í•™ìŠµ ì¤‘ë‹¨.\")\n",
    "    exit()\n",
    "\n",
    "# ì—…ë°ì´íŠ¸ëœ ëª¨ë¸ ì €ì¥\n",
    "joblib.dump(nb, \"naive_bayes_model.pkl\")\n",
    "\n",
    "# ëª¨ë¸ íŒŒì¼ í¬ê¸° í™•ì¸\n",
    "import os\n",
    "print(\"ëª¨ë¸ íŒŒì¼ í¬ê¸°:\", os.path.getsize(\"naive_bayes_model.pkl\"))\n",
    "\n",
    "print(\"ëª¨ë¸ ì—…ë°ì´íŠ¸!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      dovish       0.95      0.63      0.76       665\n",
      "     hawkish       0.77      0.97      0.86       829\n",
      "\n",
      "    accuracy                           0.82      1494\n",
      "   macro avg       0.86      0.80      0.81      1494\n",
      "weighted avg       0.85      0.82      0.81      1494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ìµœì¢…\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ë° í‰ê°€\n",
    "y_pred = nb.predict(X_test_vec)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
