{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading CSV: 1it [03:24, 204.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 분리 완료! train_data.csv & test_data.csv 생성됨\n"
     ]
    }
   ],
   "source": [
    "# 대용량 CSV 파일 경로\n",
    "input_csv = r'C:\\Users\\bona_\\infomax_crawler\\labeled_edaily.csv'\n",
    "train_csv = \"train_data.csv\"\n",
    "test_csv = \"test_data.csv\"\n",
    "\n",
    "# 청크 크기 설정\n",
    "chunksize = 5000  # 조정 가능\n",
    "\n",
    "def sentiment_label(x):\n",
    "    return ast.literal_eval(x)\n",
    "\n",
    "def make_model(nbc):\n",
    "    if not nbc:\n",
    "        return \"\", None\n",
    "    \n",
    "    tokens = []\n",
    "    target = None\n",
    "\n",
    "    for item in nbc:\n",
    "        token, freq, label = item\n",
    "\n",
    "        token_str = \"_\".join(token)\n",
    "        if token_str not in tokens:\n",
    "            tokens.append(token_str)\n",
    "\n",
    "        if label == 1:\n",
    "            target = \"hawkish\"\n",
    "        elif label == -1:\n",
    "            target = \"dovish\"\n",
    "\n",
    "    document = \" \".join(tokens) if tokens else \"\"\n",
    "    return document, target\n",
    "\n",
    "# 전체 데이터를 저장할 리스트 생성\n",
    "document_list = []\n",
    "target_list = []\n",
    "\n",
    "# CSV 파일을 청크 단위로 읽기\n",
    "for chunk in tqdm(pd.read_csv(input_csv, chunksize=chunksize), desc=\"Reading CSV\"):\n",
    "    # ngram_label 컬럼을 파싱하여 nbc 컬럼 생성\n",
    "    chunk[\"nbc\"] = chunk[\"ngram_label\"].apply(sentiment_label)\n",
    "\n",
    "    # make_model 적용 후 document, target 컬럼 생성\n",
    "    chunk[[\"document\", \"target\"]] = chunk[\"nbc\"].apply(lambda x: pd.Series(make_model(x)))\n",
    "\n",
    "    # 빈 문자열 & 중립(\"neutral\") 값 제거\n",
    "    chunk = chunk[(chunk[\"document\"] != \"\") & (chunk[\"target\"].notna())]\n",
    "\n",
    "    # 리스트에 추가\n",
    "    document_list.extend(chunk[\"document\"].tolist())\n",
    "    target_list.extend(chunk[\"target\"].tolist())\n",
    "\n",
    "# 전체 데이터 프레임 생성\n",
    "full_data = pd.DataFrame({\"document\": document_list, \"target\": target_list})\n",
    "\n",
    "# 데이터가 충분한지 확인\n",
    "if len(full_data) < 2:\n",
    "    print(\"데이터가 너무 적어 train_test_split을 실행할 수 없습니다.\")\n",
    "else:\n",
    "    # 클래스가 하나뿐이면 stratify 사용 X\n",
    "    class_counts = full_data[\"target\"].value_counts()\n",
    "    stratify_param = full_data[\"target\"] if len(class_counts) > 1 else None\n",
    "\n",
    "    # train-test split 실행\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        full_data[\"document\"], full_data[\"target\"], test_size=0.1, random_state=42, stratify=stratify_param\n",
    "    )\n",
    "\n",
    "    # DataFrame 변환\n",
    "    train_data = pd.DataFrame({\"document\": X_train, \"target\": y_train})\n",
    "    test_data = pd.DataFrame({\"document\": X_test, \"target\": y_test})\n",
    "\n",
    "    # CSV 파일 저장\n",
    "    train_data.to_csv(train_csv, index=False)\n",
    "    test_data.to_csv(test_csv, index=False)\n",
    "\n",
    "    print(\"데이터 분리 완료! train_data.csv & test_data.csv 생성됨\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 train/test 데이터 불러오기\n",
    "train_data = pd.read_csv(\"train_data.csv\")\n",
    "test_data = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "X_train = train_data[\"document\"].fillna(\"\")  # 결측값 방지\n",
    "y_train = train_data[\"target\"].fillna(\"neutral\")  # 결측값 방지\n",
    "X_test = test_data[\"document\"].fillna(\"\")\n",
    "y_test = test_data[\"target\"].fillna(\"neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_set TF-IDF 벡터화: 100%|██████████| 510/510 [00:04<00:00, 122.79it/s]\n",
      "Test_set TF-IDF 벡터화 진행: 100%|██████████| 57/57 [00:00<00:00, 154.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# TfidfVectorizer로 문서 벡터화\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,5), max_features=5000)\n",
    "X_train_vec = vectorizer.fit_transform(tqdm(X_train, desc=\"Train_set TF-IDF 벡터화\"))\n",
    "X_test_vec = vectorizer.transform(tqdm(X_test, desc=\"Test_set TF-IDF 벡터화 진행\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.pkl']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 나이브 베이즈 학습(최초)\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_vec, y_train)\n",
    "\n",
    "# 모델 저장(이후 결과 누적 위해)\n",
    "import joblib\n",
    "joblib.dump(nb, \"naive_bayes_model.pkl\")\n",
    "joblib.dump(vectorizer, \"vectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###누적학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 모델 및 벡터라이저 로드 완료\n",
      "ngram_label => document, target으로 변환 중\n",
      "새로운 데이터 개수: 410\n",
      "partial_fit 지원함. 추가 학습 진행\n",
      "모델 파일 크기: 160839\n",
      "모델 업데이트!\n"
     ]
    }
   ],
   "source": [
    "# 기존 모델 및 벡터라이저 로드\n",
    "try:\n",
    "    nb = joblib.load(\"naive_bayes_model.pkl\")\n",
    "    vectorizer = joblib.load(\"vectorizer.pkl\")\n",
    "    print(\"기존 모델 및 벡터라이저 로드 완료\")\n",
    "except FileNotFoundError:\n",
    "    print(\"기존 모델이 없음\")\n",
    "    exit()\n",
    "\n",
    "# 새로운 CSV 파일\n",
    "new_news_csv = r'C:\\Users\\bona_\\infomax_crawler\\labeled_economy.csv'\n",
    "new_df = pd.read_csv(new_news_csv)\n",
    "\n",
    "# `make_model()`을 적용하여 'ngram' -> `document`, `target` 생성\n",
    "if \"document\" not in new_df.columns:\n",
    "    if \"ngram_label\" in new_df.columns:\n",
    "        print(\"ngram_label => document, target으로 변환 중\")\n",
    "        new_df[[\"document\", \"target\"]] = new_df[\"ngram_label\"].apply(lambda x: pd.Series(make_model(ast.literal_eval(x))))\n",
    "    else:\n",
    "        print(\"'document', 'ngram_label' 없음\")\n",
    "        exit()\n",
    "\n",
    "# 데이터 확인 (중복 데이터 확인)\n",
    "new_df = new_df[(new_df[\"document\"] != \"\") & (new_df[\"target\"].notna())]\n",
    "print(\"새로운 데이터 개수:\", len(new_df))\n",
    "\n",
    "if len(new_df) == 0:\n",
    "    print(\"새로운 데이터 존재X, 학습 종료료\")\n",
    "    exit()\n",
    "\n",
    "# TF-IDF 변환 (기존 벡터라이저 사용)\n",
    "X_new = vectorizer.transform(new_df[\"document\"].fillna(\"\"))\n",
    "y_new = new_df[\"target\"].astype(str)\n",
    "\n",
    "# partial_fit 지원여부 확인인\n",
    "if hasattr(nb, \"partial_fit\"):\n",
    "    print(\"partial_fit 지원함. 추가 학습 진행\")\n",
    "    nb.partial_fit(X_new, y_new, classes=[\"hawkish\", \"dovish\"])\n",
    "else:\n",
    "    print(\"partial_fit 지원 x. 학습 중단.\")\n",
    "    exit()\n",
    "\n",
    "# 업데이트된 모델 저장\n",
    "joblib.dump(nb, \"naive_bayes_model.pkl\")\n",
    "\n",
    "# 모델 파일 크기 확인\n",
    "import os\n",
    "print(\"모델 파일 크기:\", os.path.getsize(\"naive_bayes_model.pkl\"))\n",
    "\n",
    "print(\"모델 업데이트!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      dovish       0.95      0.63      0.76       665\n",
      "     hawkish       0.77      0.97      0.86       829\n",
      "\n",
      "    accuracy                           0.82      1494\n",
      "   macro avg       0.86      0.80      0.81      1494\n",
      "weighted avg       0.85      0.82      0.81      1494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 최종\n",
    "# 테스트 데이터 예측 및 평가\n",
    "y_pred = nb.predict(X_test_vec)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
