{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZik-wgdcHos"
      },
      "source": [
        "# Ngram2Vec 실행 코드 정리\n",
        "* ngram2vec의 목적: 이를 바탕으로 한 감성사전 만들기!!!\n",
        "\n",
        "## 1. 기사에 대한 라벨링\n",
        "    1-1. 토큰화한 기사 파일\n",
        "    1-2. 품사 정리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "dHjZVQz0gKU9"
      },
      "outputs": [],
      "source": [
        "# 1. 토큰화한 기사 파일 가져오기 (test용 chunck 1만)\n",
        "import pandas as pd\n",
        "import ast\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "test = pd.read_csv(r'/content/drive/MyDrive/BOK-TEAM-2/데이터 전처리/이데일리/토큰자료/chunk_1.csv')\n",
        "test['stopwords'] = test['stopwords'].apply(lambda x: ast.literal_eval(x))\n",
        "# 2. 품사 정리해서 리스트 새로 만들기\n",
        "ALLOWED_TAGS = {'NNG', 'VA', 'VAX', 'MAG', 'negations'}\n",
        "test['stopwords'] = test['stopwords'].apply(lambda x: [word for word in x if word[1] in ALLOWED_TAGS])\n",
        "test.head()\n",
        "# 5000개에 13초 정도\n",
        "\n",
        "def processChunk(chunk_num):\n",
        "    chunk = pd.read_csv(f'/content/drive/MyDrive/BOK-TEAM-2/데이터 전처리/이데일리/토큰자료/chunk_{chunk_num}.csv')\n",
        "    chunk['stopwords'] = chunk['stopwords'].apply(lambda x: ast.literal_eval(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IPpGm03f15O"
      },
      "source": [
        "## 2. ngram 데이터 준비\n",
        "    2-1. 각 text에 대한 n-gram 추출 (1~5 gram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izGp-nBuiKUh",
        "outputId": "8d56db82-bb09-48a4-d5d6-c43a67d74ae6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [00:02<00:00, 2044.09it/s]\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "def generate_ngrams(token_list, max_n=5):\n",
        "    words = [token[0] for token in token_list]\n",
        "    ngram_counter = Counter()\n",
        "    for n in range(1, max_n + 1):\n",
        "        ngrams = [tuple(words[i:i+n]) for i in range(len(words) - n + 1)]\n",
        "        ngram_counter.update(ngrams)\n",
        "    return ngram_counter\n",
        "\n",
        "ngram_total = [generate_ngrams(text) for text in tqdm(test['stopwords'])]\n",
        "\n",
        "# 5000개 2초"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqa3kFnmf9HR"
      },
      "source": [
        "## 3. N-gram 데이터를 통해 Word2Vec 데이터 학습 모델 형성\n",
        "    3-1. 메모리를 잡아먹지 않도록 하나씩 문서를 처리하고 다음 문서를 불러옴\n",
        "    3-2. 위의 N-gram과 합쳐서 코드 적어야 함"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import ast\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# n-gram 생성 함수\n",
        "def generate_ngrams(token_list, max_n=5):\n",
        "    words = [token[0] for token in token_list]  # 단어 리스트 추출\n",
        "    ngram_list = []  # n-gram을 저장할 리스트\n",
        "    for n in range(1, max_n + 1):  # 1-gram부터 max_n-gram까지\n",
        "        ngrams = [' '.join(words[i:i+n]) for i in range(len(words) - n + 1)]  # n-gram을 문자열로 변환\n",
        "        ngram_list.extend(ngrams)  # n-gram 리스트에 추가\n",
        "    return ngram_list\n",
        "\n",
        "# Word2Vec 모델 초기화\n",
        "model = Word2Vec(vector_size=300, window=5, sg=1, negative=5, min_count=5, alpha=0.025,\n",
        "                 min_alpha=0.0001, workers=4, epochs=10)\n",
        "\n",
        "# CSV 파일 처리 함수 (각 chunk를 읽고 n-gram 리스트로 변환)\n",
        "def processChunk(chunk_num):\n",
        "    chunk = pd.read_csv(f'/content/drive/MyDrive/BOK-TEAM-2/데이터 전처리/이데일리/토큰자료/chunk_{chunk_num}.csv')\n",
        "    chunk['stopwords'] = chunk['stopwords'].apply(lambda x: ast.literal_eval(x))\n",
        "    chunk['stopwords'] = chunk['stopwords'].apply(lambda x: [word for word in x if word[1] in ALLOWED_TAGS])\n",
        "    return chunk\n",
        "\n",
        "# 첫 번째 CSV 파일에서 vocab 구축\n",
        "first_chunk = processChunk(1)  # 첫 번째 CSV 파일 읽기\n",
        "first_ngrams = []\n",
        "\n",
        "# 첫 번째 CSV에서 n-gram 생성\n",
        "for text in first_chunk['stopwords']:\n",
        "    first_ngrams.extend(generate_ngrams(text))\n",
        "\n",
        "# 모델에 vocab을 먼저 구축 (여기서 한번만 호출)\n",
        "model.build_vocab([first_ngrams], update=False)\n",
        "\n",
        "# 첫 번째 문서를 기준으로 학습 (초기 vocab 기반)\n",
        "model.train([first_ngrams], total_examples=1, epochs=model.epochs, compute_loss=True)\n",
        "\n",
        "# 문서별로 n-gram을 생성하고 모델에 추가 (반복문 최적화)\n",
        "for chunk_num in tqdm(range(2, 22)):  # 2번부터 21번까지 처리\n",
        "    df = processChunk(chunk_num)  # 각 chunk 파일 읽기\n",
        "    chunk_ngrams = []\n",
        "\n",
        "    # 각 문서별로 n-gram을 생성\n",
        "    for text in df['stopwords']:\n",
        "        chunk_ngrams.extend(generate_ngrams(text))\n",
        "\n",
        "    # 모델에 n-gram을 추가하고 학습 (한 번에 n-gram을 추가하고 학습)\n",
        "    model.build_vocab([chunk_ngrams], update=True)\n",
        "    model.train([chunk_ngrams], total_examples=1, epochs=model.epochs, compute_loss=True)\n",
        "\n",
        "# 학습된 모델 저장\n",
        "model.save(\"ngram2vec.model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IJni6LSurkkR",
        "outputId": "596f4ad3-a9ad-4960-8c18-e101ac8bc5ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/20 [00:00<?, ?it/s]WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
            "  5%|▌         | 1/20 [00:25<08:11, 25.86s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 vocabulary 확인\n",
        "vocab = model.wv.key_to_index  # 모든 단어의 목록\n",
        "print(f\"전체 vocabulary에 포함된 단어 수: {len(vocab)}\")\n",
        "print(f\"전체 vocabulary: {list(vocab.keys())[50]}\")  # 처음 10개 단어 보기\n",
        "type(list(vocab.keys())[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbT6JTwt3xD5",
        "outputId": "924ddde0-2ce3-4599-c349-8d243dd52a20"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 vocabulary에 포함된 단어 수: 73599\n",
            "전체 vocabulary: 현재\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1eiWAJSgAU1"
      },
      "source": [
        "## 4. ngram2vec 모델 테스트\n",
        "    4-1. 유사도 구해지는지 확인\n",
        "    4-2. 의문점이 생김; hawkish와 dovish를 어떻게 구분할 것인가? (Clustering? or 또 다른 정리법)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "\n",
        "# 튜플 형태로 단어가 있을 경우 벡터 평균 구하기\n",
        "def get_ngram_vector(ngram, model):\n",
        "    vectors = []\n",
        "    for word in ngram:\n",
        "        if word in model.wv:\n",
        "            vectors.append(model.wv[word])\n",
        "\n",
        "    # 벡터가 없으면 0벡터를 반환\n",
        "    if len(vectors) == 0:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "    # 벡터들의 평균을 반환\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "# 예시 n-gram\n",
        "ngram = '하락세 같 코스닥 지수'\n",
        "\n",
        "# Word2Vec 모델 로드\n",
        "# 모델은 이미 로드되어 있다고 가정\n",
        "model = gensim.models.Word2Vec.load('ngram2vec.model')\n",
        "\n",
        "# n-gram의 벡터 확인\n",
        "vector = get_ngram_vector(ngram, model)\n",
        "print(f\"'{ngram}'의 벡터: {vector}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_Lw5Cayd9SWM",
        "outputId": "74fc80f9-9ab9-48c7-a521-62155325bc8a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'하락세 같 코스닥 지수'의 벡터: [ 4.74831555e-03 -4.44567949e-03  1.03894342e-02  1.06598940e-02\n",
            "  7.67382002e-03 -4.07999046e-02 -1.36427041e-02  5.63830361e-02\n",
            "  3.48775042e-03  5.34613244e-03 -1.00443903e-02  5.47024654e-03\n",
            "  1.64841153e-02  1.31024970e-02 -2.10052636e-02 -2.01698532e-03\n",
            "  8.64014309e-03 -1.81387160e-02  1.09828413e-02 -1.34580331e-02\n",
            " -1.89270843e-02 -7.98604079e-03  2.41787955e-02  1.72139741e-02\n",
            "  3.43264639e-02 -2.46964861e-03 -3.74869891e-02  1.04097435e-02\n",
            " -1.28866937e-02 -1.91779733e-02 -1.21169481e-02  1.54829873e-02\n",
            "  1.13289803e-02  1.99607224e-03 -1.41040552e-02  1.38424207e-02\n",
            "  1.16062602e-02 -1.46797691e-02  1.55150322e-02 -7.38779223e-03\n",
            " -4.17387933e-02  5.92664233e-04  1.32148238e-02 -1.83257256e-02\n",
            "  2.46645175e-02  1.47758294e-02 -5.67368697e-03  2.31476612e-02\n",
            "  2.19245767e-03  2.31985394e-02  1.26400087e-02 -1.78823117e-02\n",
            "  5.21510188e-03 -1.19959284e-02 -9.46885534e-03 -2.86781765e-03\n",
            "  1.70562845e-02  1.40523566e-02 -1.82484258e-02  3.59073724e-03\n",
            " -5.42336795e-03  2.80938554e-03 -5.14675910e-03  2.57026739e-02\n",
            "  1.54900970e-02  1.76947657e-02  6.26690313e-03 -3.54214385e-03\n",
            " -1.51262898e-02 -1.49344876e-02  1.09928884e-02  2.52671093e-02\n",
            "  2.04906762e-02 -7.21876696e-03 -1.86824743e-02  1.41943078e-02\n",
            " -1.23773739e-02  1.55011164e-02 -1.71446428e-02  1.06776254e-02\n",
            " -2.32205726e-03 -3.06859668e-02 -8.95356853e-03  1.62962787e-02\n",
            " -2.37135366e-02 -3.15472158e-03 -2.01292578e-02 -2.28388049e-02\n",
            "  2.56071193e-03  2.39088926e-02  1.00232419e-02  4.07510716e-03\n",
            "  3.91124282e-03  3.44679388e-03  2.07588244e-02  1.54464338e-02\n",
            "  2.48145480e-02 -2.76961876e-03 -2.05199532e-02  2.72070654e-02\n",
            "  4.51087859e-03  4.70121810e-03  1.86719317e-02  3.24172378e-02\n",
            " -1.47048233e-03  1.55166292e-03 -1.41392127e-02 -1.98959522e-02\n",
            " -2.16055699e-02 -6.77447533e-03 -1.87266134e-02 -4.43975255e-03\n",
            " -5.76685509e-03  5.59330499e-03  1.64221860e-02  4.46018353e-02\n",
            "  6.46932423e-03 -8.55499320e-03 -4.97051654e-03 -2.74944790e-02\n",
            "  1.10163940e-02  4.05654795e-02  8.87584500e-03  1.29014384e-02\n",
            " -9.30103753e-03  1.72896702e-02  1.45891961e-02 -2.19433196e-02\n",
            " -1.74962319e-02  3.57396603e-02  6.37189765e-03  2.49816775e-02\n",
            "  4.97601088e-03 -9.25245788e-03  6.30063936e-03 -4.92675044e-03\n",
            " -1.39263505e-03 -5.39542967e-03 -3.33622880e-02 -5.94516471e-03\n",
            "  2.93732628e-05 -2.41646301e-02  2.10553166e-02  4.69232574e-02\n",
            "  3.19126956e-02 -3.12244110e-02 -3.68102714e-02 -3.28935799e-03\n",
            "  1.73075832e-02 -1.94066484e-02 -1.15424888e-02 -2.80278325e-02\n",
            " -1.18684014e-02 -3.77829485e-02 -1.73999649e-02  1.66151933e-02\n",
            " -4.96081486e-02  8.62044573e-04 -3.76427220e-03  3.94190885e-02\n",
            "  2.08730716e-02  1.87025107e-02 -7.07967381e-04  2.28472725e-02\n",
            "  3.38342530e-03 -1.78010594e-02  1.00167058e-02  1.82481799e-02\n",
            " -1.40327038e-02  1.95716582e-02 -2.41967607e-02  2.06483106e-04\n",
            "  3.43512893e-02  1.58123579e-03 -8.77720211e-03  3.61250676e-02\n",
            "  1.21591780e-02 -9.75370221e-03 -6.34901517e-04 -1.43172499e-02\n",
            " -4.61153220e-03  2.38890592e-02 -7.37825688e-03 -3.61947045e-02\n",
            " -1.15280654e-02 -2.27192119e-02  2.80898008e-02  2.86020692e-02\n",
            "  4.89196088e-03 -2.26885695e-02 -1.34948445e-02  2.89927833e-02\n",
            " -2.86922343e-02 -2.27253768e-03  2.35148109e-02 -2.40059309e-02\n",
            " -1.25367446e-02 -2.04703175e-02  1.77975539e-02 -1.06783863e-02\n",
            "  2.59859115e-03  5.75488154e-03  2.14194842e-02  6.77588442e-03\n",
            " -8.82932451e-03 -9.21018980e-03  4.12181765e-03 -2.23430619e-02\n",
            "  1.45696104e-02 -1.22043286e-02  1.30247455e-02 -2.81846337e-02\n",
            " -3.77368815e-02 -1.24107138e-03  1.93713196e-02 -8.09390564e-03\n",
            "  6.45444743e-05 -1.74704250e-02 -2.02281866e-02  2.28238106e-03\n",
            "  7.05274288e-03  6.26080576e-03 -1.83756500e-02 -1.56245437e-02\n",
            "  3.16218543e-03  1.32545931e-02  1.61560606e-02  4.91235754e-04\n",
            " -3.91227454e-02  1.96500663e-02  2.34984178e-02 -9.50226095e-03\n",
            "  1.24782538e-02  1.57974102e-02 -1.86931659e-02 -5.38248569e-03\n",
            "  2.72542927e-02  1.35618579e-02 -1.87799558e-02 -4.67919260e-02\n",
            " -1.77666936e-02  6.28071139e-03 -3.95618305e-02 -2.98287510e-03\n",
            " -2.44179345e-03  1.47805244e-04 -9.28183738e-03  1.94943361e-02\n",
            " -1.58676282e-02 -8.64603557e-03 -9.05627199e-03  9.39631253e-04\n",
            "  1.63383000e-02 -1.47849340e-02 -2.95608547e-02 -3.59877013e-02\n",
            "  1.73343830e-02  2.27498524e-02 -2.03030109e-02 -3.05506326e-02\n",
            "  4.49740188e-03  9.87604074e-03  8.07958003e-03 -4.01495025e-02\n",
            " -1.54101904e-02  3.33034503e-03 -1.33661795e-02  1.72253773e-02\n",
            " -1.19776297e-02 -1.54115604e-02 -2.83619277e-02  2.04207096e-02\n",
            " -6.24580588e-03  1.60155795e-03  2.43798010e-02 -7.95159116e-03\n",
            "  1.57597866e-02  2.82430742e-02 -1.87357981e-02  1.24081802e-02\n",
            "  8.71589966e-03 -1.45556778e-02  1.63059926e-03 -1.45426276e-03\n",
            "  5.91750070e-03 -1.47858297e-03 -2.88046207e-02  2.11341921e-02\n",
            " -8.69997591e-03 -1.41940964e-02  1.41015481e-02  2.78770290e-02\n",
            "  2.32489940e-04  1.35922162e-02  2.13991050e-02  3.96215878e-02\n",
            "  2.49380805e-02 -2.81432811e-02  2.68307477e-02  2.07187105e-02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import gensim\n",
        "\n",
        "model = gensim.models.Word2Vec.load('ngram2vec.model')\n",
        "# n-gram을 벡터로 변환하는 함수 (각 단어의 벡터 평균 계산)\n",
        "def get_ngram_vector(ngram, model):\n",
        "    # ngram이 튜플일 경우, 각 단어 벡터를 가져와서 평균을 구함\n",
        "    vectors = [model.wv[word] for word in ngram if word in model.wv]\n",
        "\n",
        "    if len(vectors) == 0:\n",
        "        return np.zeros(model.vector_size)  # 벡터가 없으면 0으로 대체\n",
        "\n",
        "    # 벡터들의 평균을 계산\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "# 두 n-gram 간의 유사도 계산\n",
        "def calculate_similarity(ngram1, ngram2, model):\n",
        "    vec1 = get_ngram_vector(ngram1, model)\n",
        "    vec2 = get_ngram_vector(ngram2, model)\n",
        "\n",
        "    # 벡터가 NaN인 경우를 처리\n",
        "    if np.isnan(vec1).any() or np.isnan(vec2).any():\n",
        "        return 0  # NaN이 있을 경우 유사도를 0으로 처리\n",
        "\n",
        "    # 벡터를 2D 배열로 reshape하여 cosine similarity 계산\n",
        "    similarity = cosine_similarity([vec1], [vec2])[0][0]\n",
        "    return similarity\n",
        "\n",
        "# 예시 n-gram 튜플\n",
        "ngram1 = '금리 하락' # list(vocab.keys())[500]\n",
        "ngram2 = '미국'# list(vocab.keys())[]\n",
        "\n",
        "# Word2Vec 모델 로드 (이미 학습된 모델을 사용)\n",
        "# 예시: 모델을 로드할 때 사용하는 코드 (이미 학습된 모델이 있어야 함)\n",
        "\n",
        "# 두 n-gram 튜플의 유사도 계산\n",
        "similarity = calculate_similarity(ngram1, ngram2, model)\n",
        "print(f\"'{ngram1}'와 '{ngram2}'의 유사도: {similarity}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ft02goIi3L-T",
        "outputId": "b0de0d2f-07ec-43f2-96cf-67a1d28fb3ae"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'금리 하락'와 '미국'의 유사도: 0.923205554485321\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hawkish와 dovish 사전 구해서 만들어봐야할 듯 -> 어떻게?"
      ],
      "metadata": {
        "id": "6KryGTwTC3Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9JWJp3UgC1U"
      },
      "source": [
        "## 5. 최적화\n",
        "    5-1. 우리가 처리해야 할 데이터는 text만 총 25만 개 정도!\n",
        "    5-2. 어떻게 하면 멈추거나 메모리 문제 없이 제대로 해결할 수 있을까 -> 지금 모델로는 빠르게 돌리지는 못해도 메모리 충돌이나 과다는 없었음\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swaA7_FTcA-h"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}